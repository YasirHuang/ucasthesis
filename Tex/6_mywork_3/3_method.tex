\section{方法描述}
\label{sec:5_method}

对于一个标准的图片信息辅助式神经机器翻译，其输入为一个源语言句子$X$和一个与其对应的图片$I$，然后利用这两个模态的信息生成翻译$Y$。然而，现有的很多方法对输入图片$I$中的信息并不敏感。这是因为翻译任务所使用的语料中，大部分$X$与$Y$之间有着很好的语义对齐关系。这使得翻译模型在不依靠图片输入的情况下，就可以在训练阶段学习到一组相对不错的模型参数，使模型很容易收敛到一个不需要图片信息的局部最优解。为了使模型充分利用为待翻译句子所提供的视觉信息，本章提出了基于图文对比对抗训练的神经机器翻译方法。本节将首先介绍本章所使用的翻译模型结构，然后详细介绍如何通过对比对抗训练增加模型对视觉信息的敏感度。

\subsection{模型结构}
\label{sec:5_architecture}
本章采用了一种句子级跨模态语义融合模型结构，其输入为一个文本序列，和一个可选的视觉输入，该视觉输入为文本序列所对应的完整图片以及图片内部的数个视觉目标，如图【】所示。该模型的编码器和解码器为一般的Transformer结构。其中，Transformer的编码器负责跨模态信息融合。为了使Transformer支持文本加图片形式的输入，我们采用了一个多模态嵌入层。该嵌入层共分为4个子层：词嵌入层，视觉特征层，模态分割层，位置编码层。

（1）{\sffamily 词嵌入层：}为了支持图片的输入，需要在词嵌入层所对应的词表中，加入一些特殊字符：“<seg>”将词嵌入层输入的前后两部分分割为文本和视觉两个序列；“<img>”代表着放置完整图片的位置，每个输入序列仅一个；“<bbx>”代表放置图片中的视觉目标的位置，每个输入序列可以有多个；“<end>”代表着多模态输入序列的结尾。


（2）{\sffamily 视觉特征层：}该层每个位置的输入与词嵌入层的特殊字符相对应，其中“<img>”的对应位置放置输入的完整图片的全局特征，“<bbx>”对应位置放置视觉目标的图片全局特征，其它位置则输入零向量。


（3）{\sffamily 分割层：}这一层的作用相对简单，主要用作标识每个位置的输入属于文本序列还是图片序列。因此一共只有“文本模态”和“视觉模态”两个值，每个在模型中是一个与词嵌入层维度相同的向量。其中“<seg>”之前的文本序列均对应着“文本模态”，“<seg>”以及其后面的序列均为“视觉模态”。


（4）{\sffamily 位置向量层：}这一层与一般的位置向量作用相似，能够表示输入序列中的绝对位置关系。区别在于，当输入的视觉目标与文本中的某些词有对应关系时，可以将视觉目标的位置设置为与其对应的词相同的位置，从而达到加强图片信息作用准确性的目的。如图【】所示，图片输入序列中的“帽子”的绝对位置为5，与文本序列中的“hat”保持一致。

在解码阶段，解码器仅接收“文本模态”的编码结果作为输入。这是因为本章的CAT方法旨在帮助神经机器翻译模型将视觉信息融合到文本表示中。而该过程发生在编码阶段，即没有针对解码器采取翻译以外的优化方法。因此，基于一般的翻译模型对视觉信息不敏感的假设，其内部模块，如解码器，往往也会忽略视觉信息的输入。所以，将“视觉模态”部分的隐层单元传递给解码器是多余的。

图【】中可以看到，CAT-MMT的目标函数包含了两个部分：融合图片信息的神经机器翻译所需的交叉熵损失函数和CAT所需的对比损失函数。其中交叉熵损失函数为：
\begin{equation}
    L_{CE}(\phi, \theta, \psi)=-\sum_j^M \log p(y_j|y_{<j},X,I)
\label{eq:5_cross_entropy}
\end{equation}
其中，$y_j \in Y,j=1,\cdots,M$，$\phi$为解码器参数，$\theta$为编码器参数，$\psi$为多模态嵌入层的参数。该损失函数与一般的融合图片信息的神经机器翻译模型的相似，均是通过源语言文本$X$和对应图片$I$为输入信息，生成翻译$Y$的形式。在我们的方案中，$I$代表了完整的图片与其中的多个视觉目标的组合。该损失函数无法反映出的信息是，本章的跨模态信息融合过程，仅发生在编码端。图【】中除了平均池化(average pooling)层，其它参数均需要通过翻译任务来优化。

\subsection{对比对抗训练}
\label{sec:5_cat}
正如\ref{sec:5_architecture}小节所介绍，本章是在Transformer的编码器中实现将跨模态信息融合到文本的表示中。然而，基于一般的翻译模型对视觉信息不敏感的假设，在没有额外的引导下，编码器同样会忽略图片信息的存在。为了有效地融合视觉信息，本章提出了图文对比对抗方法来实现在编码过程将图片中的视觉信息融合到文本的表示中。CAT方法主要包含三部分：对比学习，对抗样本以及双向翻译训练。

{\sffamily 对比学习}

对比学习方法能够在语义表示空间中拉近相似的样本，推离不相关的样本。针对融合图片信息神经翻译中，将目标译文$Y$作为锚点，将编码端的输入$X+I$作为正样本。在$X$与$Y$的双语统一表示空间中，对比学习方法能够拉近$X+I$与$Y$之间的距离，并将其它的所有样本视为负样本，拉开与$X+I$和$Y$之间的距离。图【】中每个圆代表文本语义表示空间的一个点，其中图【】(a)和图【】(b)展示了对比学习将表示空间的样本分簇归类的能力，图【】(b)中的每个簇可视为一个双语平行句对。常规的对比学习损失函数为：
\begin{equation}
    \mathcal{L}_{CL}(\theta, \psi)=-\log\ \frac{e^{\mathrm{sim}(\mathcal{R}(Y),\mathcal{R}(X+V))/\tau}}{\sum_{Z\in N}e^{\mathrm{sim}(\mathcal{R}(Y),\mathcal{R}(Z))/\tau}}
    \label{eq:5_contrastive_learning}
\end{equation}

其中$\mathrm{sim}(\cdot,\cdot)$为余弦相似度函数，用于计算两个表示向量的语义相似度；$\tau$为温度，控制区分正负样本的能力；$\mathcal{R}(\cdot)$代表平均池化，正如图【】所示，对比损失的输入是文本表示经平均池化后的结果；$N$代表着负样本集。

$\mathcal{L}_{CL}$与$\mathcal{L}_{CE}$之间共享编码器和嵌入层的参数，因此本章所采用的对比学习只针对编码器进行参数优化。