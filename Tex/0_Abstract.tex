%大背景
% 先提出时代背景
近年来，端到端的神经机器翻译方法取得了飞速的发展。相比于传统的统计机器翻译方法，在翻译质量方面有显著提升的同时，在融合跨模态信息方面也凸显出其独有的优势。
% 再提出任务意义：为什么融合图片信息
融合图片信息的神经机器翻译方法就是在基于编码器-解码器框架的翻译模型中，通过利用图片中的视觉信息改善文本翻译质量的一类方法。
%图片中往往包含着比文本更丰富、更完整或更准确的信息，
图片中往往蕴含着文本以外的额外信息，能够补充或强调文本信息，% from yzhao
因此在句子编码过程中加入图片信息以完善文本的表示，或是在解码过程中提供图片信息作为参考以指导译文的生成，都是图片能够为翻译带来的增益效果的有效途径。
%尽管神经网络方法能够直接从数据中学习语言之间的对齐关系，但是纯文本翻译中仍然存在着一些挑战和局限性，
% 存在哪些问题，大的问题，
尽管图片对于机器翻译的过程有着重要作用，但是在神经机器翻译中融合图片信息依然面临着多项挑战，例如跨模态对齐关系弱，使得翻译模型倾向于丢弃图片信息而退化为常规的纯文本翻译模型；或是图片信息利用难，在翻译模型中输入图片或者噪音带来的翻译质量提升效果差距很小；以及图片信息作用不明确，这使得模型的改进难以具有针对性。
%虽然神经网络方法能够直接从数据中提取特征，但是跨模态的信息融合与利用还有一些挑战。
%双语数据在翻译任务中具有高度的对齐性，而图片与文本之间的跨模态对齐则相对较弱。因此，如果将图片直接输入给常规的神经机器翻译模型，可能会导致模型在训练过程中忽略跨模态信息，而只关注更容易学习的纯文本翻译。

% 本文所围绕的问题
本文围绕如何设计有效的图片与文本之间的跨模态信息融合方法提升神经机器翻译的质量展开研究，如通过明确图片信息在文本中的作用目标来规避模型对图片信息不敏感的问题，或强化图片信息在模型训练过程中的作用来提升模型对图片信息的敏感度。论文的主要工作和创新点归纳如下：

{\sffamily 1. 提出了一种基于跨模态文本重构的神经机器翻译方法}

在神经机器翻译中融合图片信息的常规方法采用隐式跨模态信息融合方法，其将图片和句子一同输入到翻译模型中参与编码或解码过程，从而使图片信息与文本信息充分地融合。然而，这类方法存在图片信息的作用方式不明确的问题。% from yzhao
%将图片输入到神经机器翻译模型中具有直接从数据中学习并融合跨模态信息的优点，但也难以明确图片信息的具体作用，因此这类方法可称为隐式跨模态信息融合法。
为了探究显式跨模态信息融合法是否可行，本文提出一种基于跨模态文本重构的神经机器翻译方法。该方法在训练中将源语言句子中的名词或短语的位置显式地替换为图片中对应的视觉目标，并将该序列输入到重构模型中用于生成原来的或目标语言的句子。最后通过参数共享的方式将重构模型的参数与翻译模型共享，达到提升翻译质量的目的。实验表明，本文所提方法在测试阶段不需要输入图片的情况下达到与隐式方法可比的翻译准确率。并且该方法主要提升了与视觉目标相对应的实体词的翻译准确率。

{\sffamily 2. 提出了一种基于双向跨模态实体重构的神经机器翻译方法}

显式跨模态信息融合法能够准确地将图片信息作用到目标词上，但是采用文本重构一方面仅应用了图像到文本单方向重构，另一方面视觉信息仅作用到了实体词上。为了在显式方法中更充分地利用图片信息，并融合隐式方法的优点，本文提出一种基于双向跨模态实体重构的神经机器翻译方法。
%该方法抛弃了文本级别的重构，在文本实体和视觉实体之间做双向重构。并增加了非实体的重构，使图片信息与文本上下文做进一步的信息融合。
不同于之前方法进行文本级别的重构，本方法在文本实体和视觉实体之间做双向重构。同时，为了更进一步地在文本上下文中融合图片信息，还增加了文本非实体的重构。%from yzhao
然后，将以上三种重构任务与翻译任务通过多任务学习的方式结合。实验表明，该方法在测试阶段不需要输入图片的情况下进一步地提升了机器翻译的质量。实验分析表明，双向实体重构与非实体重构的多任务组合方式使模型受益最大。

{\sffamily 3. 提出了一种基于图文对比对抗训练的神经机器翻译方法}

显式跨模态信息融合方法能够优化神经机器翻译模型的表示能力，进而提升模型的翻译质量。然而针对句子中有歧义词或语义不完整等问题时，则需要将图片输入到翻译模型中，利用图片中的额外信息辅助翻译过程，从而得到更准确的译文。但是此类隐式方法普遍存在视觉信息在模型中难以起作用的问题。
%虽然显式跨模态信息融合方法能够为神经机器翻译模型带来翻译质量的提升。但此类方法存在图片信息利用不充分的问题，而隐式方法普遍存在视觉信息在模型中难以起作用的问题。
为此，本文提出一种基于图文对比对抗训练的神经机器翻译方法。为了拉近双语的语义关系，在编码端增加了图文与目标语言句子之间对比学习。并在负样本集中引入了包含源语言句子与错误图片组合而成的对抗样本。为了将正负样本区分开，模型需要判断图片信息是否与源语言句子的语义一致。该方法会将图片信息融合到文本的表示中，从而提升视觉信息在模型中的作用程度。实验表明，在提升了翻译准确率的同时，所提方法较输入错误图片或不输入图片的情况翻译质量明显提升。

综上所述，本文旨在设计更好的图片信息融合方法，提升图片信息在神经机器翻译模型中的作用效果。为此，本文设计了显式的跨模态信息融合方法、隐式跨模态信息融合方法以及两种方式相结合的方法。实验表明，本文所提方法能够有效地将图片信息融合到翻译模型中，并为翻译质量带来提升。

\keywords{神经机器翻译，跨模态信息融合，多任务学习，对比学习}% 中文关键词
%-
%-> 英文摘要
%-
\intobmk\chapter*{Abstract}% 显示在书签但不显示在目录

End-to-end neural machine translation methods have evolved rapidly in recent years. Compared with traditional statistical machine translation methods, it not only has significantly improved translation quality but also highlights its unique advantages in cross-modal information fusion. The neural machine translation method of fusing image information is a kind of method to improve the quality of text translation by using the visual information from the image in the translation model based on the encoder-decoder framework. Images often contain extra information than text and are able to complete texual information or highlight the important part in texts. Therefore, image information is added in the sentence encoding process to improve the text representation, or provided as a reference in the decoding process to guide the generation of translations. These are the effective ways that images can bring benefits to translations. Although image information is important for translation processes, there are still some challenges in the fusion and utilization of information across modalities. 
For example, the cross-modal alignment relationship between sentences and images is weak, so the translation model tends to discard image information and degenerates into a conventional text-only translation model, or image information is challenging to use, and the difference in the improved translation quality caused by between inputting image or noise is minimal; and the role of image information is not clear, which makes it challenging to improve the model in a specific manner.

This paper focuses on how to design an effective cross-modal information fusion method between images and texts to improve the quality of neural machine translation, such as avoiding the problem of model insensitivity to image information by clarifying the role of image information in text, or strengthening the role of image information in the model training process to improve the sensitivity of the model to image information. The main work and innovations of the paper are summarized as follows:

\textbf{1. Neural machine translation based on cross-modal text reconstruction}

The conventional methods of fusing image information in neural machine translation adopt implicit cross-modal information fusion methods, which inputs images and sentences into the translation model to participate in the encoding or decoding process, so that the image information and text information are fully fused. However, this type of method suffers from unknowing how the picture information works. In order to explore whether explicit cross-modal information fusion is feasible, this paper proposes a neural machine translation method based on cross-modal text reconstruction. During training, the method explicitly replaces the positions of nouns or phrases in the source language sentences with the corresponding visual objects in the images, and inputs the sequence into the reconstruction model to generate sentences into the original or target languages. Finally, the parameters of the reconstruction model are shared with the translation model through parameter sharing to achieve the purpose of improving translation quality. Experiments show that the method proposed in this paper achieves translation accuracy comparable to implicit methods without requiring input images during test. And this method mainly improves the translation accuracy of entity words corresponding to visual targets.

\textbf{2. Neural machine translation based on bidirectional cross-modal entity reconstruction}

The explicit cross-modal information fusion method can accurately apply image information to target words. But on the one hand, text reconstruction only applies an unidirectional image-to-text reconstruction, and on the other hand, visual information only acts on entity words. In order to make full use of image information in an explicit way and combine the advantages of implicit methods, this paper proposes a neural machine translation method based on bidirectional cross-modal entity reconstruction. Unlike previous methods that perform text-level reconstruction, this method performs bidirectional reconstruction between textual entities and visual entities. At the same time, in order to further fuse image information into the text context, the text none-entity reconstruction is also applied. Then, the above three reconstruction tasks are combined with the translation task through multi-task learning. Experiments show that this method further improves the quality of machine translation without requiring input images in the testing phase. Experimental analysis shows that the multi-task combination of bidirectional entity reconstruction and non-entity reconstruction benefits the model the most.

\textbf{3. Neural machine translation based on image-text contrastive adversarial training}

The explicit cross-modal information fusion methods can optimize the representation ability of the neural machine translation models, thereby improving the translation quality. However, when there are ambiguous words or incomplete semantics in the sentences, it is necessary to input images into the translation model and use the additional information in images to guide the translation process, so as to obtain a more accurate translation. However, such implicit methods generally have the problem that visual information is difficult to function in the model. To this end, this paper proposes a neural machine translation method based on image-text contrastive adversarial training. In order to narrow the semantic relationship between bilinguals, contrastive learning between image-text pairs and sentences in the target language is applied in the encoding stage. And adversarial samples containing source language sentences combined with wrong images are introduced into the negative sample set. In order to distinguish positive and negative samples, the model needs to judge whether the image information is consistent with the semantics of the source language sentence. This method will fuse image information into text representations, thereby improving participant of visual information in the model. Experiments show that while improving the translation accuracy, the proposed method significantly improves the translation quality compared with inputting wrong or not inputting images.

To sum up, this paper aims to design better image information fusion methods to improve the usage of image information in neural machine translation models. To this end, this paper designs an explicit cross-modal information fusion method, an implicit cross-modal information fusion method, and a combination of the two methods. Experiments show that the method proposed in this paper can effectively fuse image information into the translation model and improve translation quality.

\KEYWORDS{Neural Machine Translation，Cross-modal Information Fusion，Multi-task Learning, Contrastive Learning}