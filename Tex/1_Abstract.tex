%大背景
% 先提出时代背景
近年来，端到端的神经机器翻译方法取得了飞速的发展。相比于传统的统计机器翻译方法，在翻译质量方面有显著提升的同时，在融合跨模态信息方面也凸显出其独有的优势。
% 再提出任务意义：为什么融合图片信息
融合图片信息的神经机器翻译方法就是一种在基于编码器-解码器框架的翻译模型中，通过利用图片中的视觉信息改善文本翻译质量的一类方法。图片中往往包含着比文本更丰富、更完整或更准确的信息，因此在句子编码过程中加入图片信息以完善文本的表示，或是在解码过程中提供图片信息作为参考以指导译文的生成，都是图片能够为翻译带来的增益效果的有效途径之一。
%尽管神经网络方法能够直接从数据中学习语言之间的对齐关系，但是纯文本翻译中仍然存在着一些挑战和局限性，
% 存在哪些问题，大的问题，
虽然神经网络方法能够直接从数据中提取特征，但是跨模态的信息融合与利用还有一些挑战。双语数据在翻译任务中具有高度的对齐性，而图片与文本之间的跨模态对齐则相对较弱。因此，如果将图片直接输入给普通的神经机器翻译模型，可能会导致模型在训练过程中忽略跨模态信息，而只关注更容易学习的跨语言对齐。

% 本文所围绕的问题
本文围绕如何设计有效的图片与文本之间的跨模态信息融合方法提升神经机器翻译的质量，如通过明确图片信息在文本中的作用目标来规避模型对图片信息不敏感的问题，或强化图片信息在模型训练过程中的作用来提升模型对图片信息的敏感度。论文的主要工作和创新点归纳如下：

{\sffamily 1. 提出了一种基于跨模态文本重构的神经机器翻译方法}

将图片输入到神经机器翻译模型中具有直接从数据中学习并融合跨模态信息的优点，但也难以明确图片信息的具体作用，因此这类方法可称为隐式跨模态信息融合法。为了探究显式跨模态信息融合法是否可行，本文提出一种基于跨模态文本重构的神经机器翻译方法。该方法在训练中将源语言句子中的名词或短语的位置明确地替换为图片中对应的视觉目标，并将该序列输入到重构模型中用于生成原来的或目标语言的句子。最后通过参数共享的方式将重构模型的参数与翻译模型共享，达到提升翻译质量的目的。实验表明，本文所提方法在测试阶段不需要输入图片的情况下达到与隐式方法可比的翻译准确率。并且该方法主要提升了与视觉目标相对应的实体词的翻译准确率。

{\sffamily 2. 提出了一种基于双向跨模态实体重构的神经机器翻译方法}

显式跨模态信息融合法能够准确地将图片信息作用到目标词上，但是采用文本重构一方面仅应用了图像到文本单方向重构，另一方面视觉信息仅作用到了实体词上。为了在显式方法中更充分地利用图片信息，并融合隐式方法的优点，本文提出一种基于双向跨模态实体重构的神经机器翻译方法。该方法抛弃了文本级别的重构，在文本实体和视觉实体之间做双向重构。并增加了非实体的重构，使图片信息与文本上下文做进一步的信息融合。然后，将以上三种重构任务与翻译任务通过多任务学习的方式结合。实验表明，该方法在测试阶段不需要输入图片的情况下进一步地提升了机器翻译的质量。实验分析表明，双向实体重构与非实体重构的多任务组合方式使模型受益最大。

{\sffamily 3. 提出了一种基于图文对比对抗训练的神经机器翻译方法}

虽然显式跨模态信息融合方法能够为神经机器翻译模型带来翻译质量的提升。但此类方法存在图片信息利用不充分的问题，而隐式方法普遍存在视觉信息在模型中难以起作用的问题。为此，本文提出一种基于图文对比对抗训练的神经机器翻译方法。为了拉近双语的语义关系，在编码端增加了图文与目标语言句子之间对比学习。并在负样本集中引入了包含源语言句子+错误图片对抗样本。为了将正负样本区分开，模型需要判断图片信息是否与源语言句子的语义一致。该方法会将图片信息融合到文本的表示中，从而提升视觉信息在模型中的作用程度。实验表明，在提升了翻译准确率的同时，所提方法在输入错误图片或不输入图片的情况下翻译质量明显下降。

综上所述，本文旨在设计更好的图片信息融合方法，提升图片信息在神经机器翻译模型中的作用效果。为此，本文设计了显式的跨模态信息融合方法、隐式跨模态信息融合方法以及两种方式相结合的方法。实验表明，本文所提方法能够有效地将图片信息融合到翻译模型中，并为翻译质量带来提升。

\keywords{神经机器翻译，跨模态信息融合，多任务学习，对比学习}% 中文关键词
%-
%-> 英文摘要
%-
\intobmk\chapter*{Abstract}% 显示在书签但不显示在目录

End-to-end neural machine translation methods have evolved rapidly in recent years. Compared with traditional statistical machine translation methods, it not only has significantly improved translation quality but also highlights its unique advantages in integrating cross-modal information. The neural machine translation method of fusing image information is a kind of method to improve the quality of text translation by using the visual information in the image in the translation model based on the encoder-decoder framework. Pictures often contain richer, more complete or more accurate information than text. Therefore, picture information is added in the sentence encoding process to improve the text representation, or picture information is provided as a reference in the decoding process to guide the generation of translations. It is one of the effective ways that pictures can bring benefits to translation. Although neural network methods can directly extract features from data, there are still some challenges in the fusion and utilization of information across modalities. Bilingual data has a high degree of alignment in translation tasks, while the cross-modal alignment between images and text is relatively weak. Therefore, if images are directly fed to a common neural machine translation model, it may cause the model to ignore cross-modal information during training and only focus on easier-to-learn cross-lingual alignments.

This paper focuses on how to design an effective cross-modal information fusion method between pictures and texts to improve the quality of neural machine translation, such as avoiding the problem of model insensitivity to picture information by clarifying the role of picture information in text, or strengthening pictures The role of information in the model training process to improve the sensitivity of the model to image information. The main work and innovations of the paper are summarized as follows:

\textbf{1. Neural machine translation based on cross-modal text reconstruction}

Inputting pictures into neural machine translation models has the advantage of directly learning from data and fusing cross-modal information, but it is also difficult to clarify the specific role of picture information, so this type of method can be called implicit cross-modal information fusion. In order to explore whether explicit cross-modal information fusion is feasible, this paper proposes a neural machine translation method based on cross-modal text reconstruction. During training, the method explicitly replaces the positions of nouns or phrases in the source language sentences with the corresponding visual objects in the pictures, and inputs this sequence into the reconstruction model to generate sentences in the original or target language. Finally, the parameters of the reconstructed model are shared with the translation model through parameter sharing, so as to improve the translation quality. Experiments show that the method proposed in this paper achieves translation accuracy comparable to implicit methods without requiring input images during the test phase. And this method mainly improves the translation accuracy of entity words corresponding to visual targets.

\textbf{2. Bidirectional cross-modal entity reconstruction based neural machine translation}

The explicit cross-modal information fusion method can accurately apply image information to target words, but using text reconstruction, on the one hand, it only applies image-to-text unidirectional reconstruction, and on the other hand, visual information only acts on entity words. In order to make full use of image information in explicit methods and combine the advantages of implicit methods, this paper proposes a neural machine translation method based on bidirectional cross-modal entity reconstruction. This method abandons text-level reconstruction, and does bidirectional reconstruction between textual entities and visual entities. In addition, non-entity reconstruction is added to further fuse image information and text context. Then, the above three reconstruction tasks are combined with the translation task through multi-task learning. Experiments show that this method further improves the quality of machine translation without requiring input images in the testing phase. Experimental analysis shows that the multi-task combination of bidirectional entity reconstruction and non-entity reconstruction benefits the model the most.

\textbf{3. Neural Machine Translation Based on Graphic-Text Contrastive Adversarial Training}

Although explicit cross-modal information fusion methods can bring translation quality improvements to neural machine translation models. However, such methods have the problem of insufficient utilization of image information, and implicit methods generally have the problem that visual information is difficult to play a role in the model. To this end, this paper proposes a neural machine translation method based on image-text contrastive training. In order to narrow the semantic relationship between bilinguals, a comparative study between pictures and texts and sentences in the target language is added on the encoding side. And adversarial samples containing source language sentences + wrong pictures are introduced in the negative sample set. In order to distinguish positive and negative samples, the model needs to judge whether the image information is consistent with the semantics of the source language sentence. This method will integrate image information into text representation, thereby improving the role of visual information in the model. Experiments show that while the translation accuracy is improved, the translation quality of the proposed method decreases significantly when the wrong picture is input or no picture is input.

To sum up, this paper aims to design a better image information fusion method to improve the effect of image information in neural machine translation models. To this end, this paper designs an explicit cross-modal information fusion method, an implicit cross-modal information fusion method and a combination of the two methods. Experiments show that the method proposed in this paper can effectively integrate image information into the translation model and improve translation quality.

\KEYWORDS{Neural Machine Translation，Cross-modal Information Fusion，Multi-task Learning, Contrastive Learning}