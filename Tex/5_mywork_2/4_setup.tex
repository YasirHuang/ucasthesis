\section{实验设置}

\subsection{数据}

本章使用多模态机器翻译任务的常用数据集Multi30K来测试本文所提方法。该数据集每张图片配有一个英文句子和其对应的德语和法语的译文。该数据集的训练集包含29000个平行句对。其验证集和测试集Test2016的大小分别为1014和1000。本文还在更新的Multi30K Test2017以及Ambiguous MSCOCO上进行了测试，其大小分别为1000和461。

本文采用Moses SMT$^{[29]}$工具包对文本数据进行分词(Tokenization）和归一化(Normalization）处理。为了防止对视觉实体与文本实体对应关系的破坏，本文并没有采用双字节编码(Bytepairencoding，BPE）进行分词处理$^{[30]}$。在数据的预处理阶段，如图1所示，本文采用spaCy\footnote{https://spacy.io}为提取名词短语的工具。视觉目标检测工具$^{[28]}$与文献${[26]}$采用了相同的单步文本$-$视觉定位方案。最后将视觉实体输入Resnet-50$^{[27]}$提取出2048维的全局图像特征，该特征向量在输入到CER模型的时候会通过一个全连接层映射到模型所需的128维。

\subsection{模型参数}
本文在基于Transformer$^{[24]}$结构的模型上进行试验。由于数据集Multi30K的规模相对较小，很容易在Transformer-big或Transformer-base上过拟合，因此本文采用了更小规模的参数设置。本文所设置的参数与文献${[26]}$基本保持一致，其词嵌入层设置为128维，前馈内层为256维。编码器、视觉实体解码器和文本解码器各4层，多头自注意力的头数为4。词表采用源语言与目标语言共享的方式，合并后英译德的共享词表大小为27226，英译法为19393，英译捷为29170。利用Adam$^{[31]}$优化器优化整个模型参数，并在优化80000步后停止训练得到最终模型参数。训练时，批数据的大小设置为2000个词。测试时，采用了搜索空间$b=4$的柱搜索算法。

多任务学习方法的训练方式为随机选择一个任务在当前批数据下对模型进行优化。本文所提CER方法一共包含三个子任务:VER、TER和TNER。这三个任务根据超参数$\alpha$、$\beta$和$\gamma$控制训练的比例。本文将三个子任务的训练比例设置为$\alpha=40\%$、$\beta=40\%$和$\gamma=20\%$。这样保证了CER在训练中以$80\%$的比例用于跨模态实体重构，$20\%$的比例用于视觉实体与非实体上下文的语义融合。本文将NMT设置为主任务，并设置超参数$\omega$为控制NMT与CER之间的训练比重。例如当$\omega=0.5$时，NMT与CER的训练比例分别为$50\%$，其中VER、TER和TNER三个子任务的训练比重依据上文调整为$\omega\times\alpha=20\%$、$\omega\times\beta=20\%$和$\omega\times\gamma=10\%$。

本文所有的模型结果都是在3次随机初始化的条件下训练得到的平均值。最后，通过BLEU4$^{[32]}$和METEOR$^{[33]}$来测试翻译质量，在后面的实验中分别简化为``B''和``M''。


\subsection{对比模型}
本文将MMT方法分为句子级融合、视觉实体融合以及增强NMT三大类。句子级融合方法主要尝试将视觉信息与整个文本句子进行语义融合。视觉实体融合方法则提预先提取出图片中的视觉目标后再输入到所设计的模型中。增强NMT方法一般在训练中融合视觉信息，而在测试时不需要输入图片。后两类方法中的多数模型在本质上也属于句子级融合方法。各模型简介如下:

%\begin{itemize}
%\item
1）Base是标准句子级Transformer翻译模型$^{[24]}$。具体参数见3。2小节。

%\item
2）IMGD$^{[4]}$是一种基于GRU$^{[34]}$的翻译模型，利用图像全局特征初始化解码器。

%\item
3）${\rm VMMT_C}$$^{[8]}$是一种基于LSTM$^{[35]}$的翻译模型，在翻译过程中利用视觉信息与文本利用变分编码器融合跨模态语义。${\rm VMMT_F}$在${\rm VMMT_C}$的基础上没有使用图像。

%\item
4）SerAttTrans$^{[11]}$在解码端串行使用两个交叉注意力模块分别用于关注源端文本信息和图像局部特征。

%\item
5）GumAttTrans$^{[36]}$使用Gumbel$-$Sigmoid改造注意力机制，帮助翻译模型关注到图片中与文本内容更相关的区域。

%\item
6）parallel RCNNs$^{[13]}$将每个提取出的视觉实体分别与源语言文本进行先拼接再编码的过程，得到的并行的源端编码序列将用于目标端的解码。

%\item
7）GMMT$^{[26]}$模型视源语言句子与对应图片中的视觉目标的关系为一个多模态图结构，然后利用专门的基于图的跨模态编码器进行编码，最终解码出目标端句子$^{[26]}$。

%\item
8）DelMMT$^{[37]}$模型提出使用推敲网络做多模态二次解码。在第二次解码中融合源语言、目标语言以及视觉信息。

%\item
9）Imagination$^{[5]}$是一种基于GRU的翻译模型，并采用多任务的方式在附属任务中利用源语言文本的编码结果生成完整图片的全局特征。同本文所提方法相似，其在测试过程中不需要输入图片。

%\item
10）EMMT$^{[18]}$采用退化文本与视觉目标的组合重建源端或目标端的句子，同样在测试阶段不需要输入图片。因为该方法输入的图片为视觉目标，因此也可以归类为视觉实体融合类方法。
%\end{itemize}