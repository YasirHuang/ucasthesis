%---------------------------------------------------------------------------%
%-                                                                         -%
%-                             Bibliography                                -%
%-                                                                         -%
%---------------------------------------------------------------------------%
@book{wikibook2014latex,
    title={http://en.wikibooks.org/wiki/LaTeX},
    author={Wikibook},
    year={2014},
    publisher={On-line Resources}
}
@book{lamport1986document,
    title={Document Preparation System},
    author={Lamport, Leslie},
    year={1986},
    publisher={Addison-Wesley Reading, MA}
}
@article{chen2005zhulu,
    title={著录文后参考文献的规则及注意事项},
    author={陈浩元},
    key={Chen Hao Yuan},
    journal={编辑学报},
    volume={17},
    number={6},
    pages={413--415},
    year={2005}
}
@book{chu2004tushu,
    title={图书馆数字参考咨询服务研究},
    author={初景利},
    key={Chu Jing Li},
    year={2004},
    address={北京},
    publisher={北京图书馆出版社}
}
@article{stamerjohanns2009mathml,
    title={{MathML}-aware article conversion from {LaTeX}},
    author={Stamerjohanns, Heinrich and Ginev, Deyan and David, Catalin and Misev, Dimitar and Zamdzhiev, Vladimir and Kohlhase, Michael},
    journal={Towards a Digital Mathematics Library},
    volume={16},
    number={2},
    pages={109--120},
    year={2009},
    publisher={Masaryk University Press}
}
@article{betts2005aging,
    title={Aging reduces center-surround antagonism in visual motion processing},
    author={Betts, Lisa R and Taylor, Christopher P},
    journal={Neuron},
    volume={45},
    number={3},
    pages={361--366},
    year={2005},
    publisher={Elsevier}
}

@article{bravo1990comparative,
    title={Comparative study of visual inter and intrahemispheric cortico-cortical connections in five native Chilean rodents},
    author={Bravo, Hermes and Olavarria, Jaime},
    journal={Anatomy and embryology},
    volume={181},
    number={1},
    pages={67--73},
    year={1990},
    publisher={Springer}
}
@book{hls2012jinji,
    author       = {哈里森·沃尔德伦},
    key          = {Haliseng Woerdelun},
    translator   = {谢远涛},
    title        = {经济数学与金融数学},
    address      = {北京},
    publisher    = {中国人民大学出版社},
    year         = {2012},
    pages        = {235--236},
}
@proceedings{niu2013zonghe,
    editor       = {牛志明 and 斯温兰德 and 雷光春},
    key          = {Niu Zhi Ming Siwenlande Lei Guang Chun},
    title        = {综合湿地管理国际研讨会论文集},
    address      = {北京},
    publisher    = {海洋出版社},
    year         = {2013},
}
@incollection{chen1980zhongguo,
    author       = {陈晋镳 and 张惠民 and 朱士兴 and 赵震 and
        王振刚},
    key          = {Chen Jing Ao Zhang Hui Ming Zhu Shi Xing Zhao Zhen Wang Zhen Gang},
    title        = {蓟县震旦亚界研究},
    editor       = {中国地质科学院天津地质矿产研究所},
    booktitle    = {中国震旦亚界},
    address      = {天津},
    publisher    = {天津科学技术出版社},
    year         = {1980},
    pages        = {56--114},
}
@article{yuan2012lana,
    author       = {袁训来 and 陈哲 and 肖书海},
    key          = {Yuan xun lai Chen zhe Xiao shu Hai},
    title        = {蓝田生物群: 一个认识多细胞生物起源和早期演化的新窗口 -- 篇一},
    journal      = {科学通报},
    year         = {2012},
    volume       = {57},
    number       = {34},
    pages        = {3219},
}
@article{yuan2012lanb,
    author       = {袁训来 and 陈哲 and 肖书海},
    key          = {Yuan xun lai Chen zhe Xiao shu Hai},
    title        = {蓝田生物群: 一个认识多细胞生物起源和早期演化的新窗口 -- 篇二},
    journal      = {科学通报},
    year         = {2012},
    volume       = {57},
    number       = {34},
    pages        = {3219},
}
@article{yuan2012lanc,
    author       = {袁训来 and 陈哲 and 肖书海},
    key          = {Yuan xun lai Chen zhe Xiao shu Hai},
    title        = {蓝田生物群: 一个认识多细胞生物起源和早期演化的新窗口 -- 篇三},
    journal      = {科学通报},
    year         = {2012},
    volume       = {57},
    number       = {34},
    pages        = {3219},
}
@article{walls2013drought,
    author       = {Walls, Susan C. and Barichivich, William J. and Brown, Mary
        E.},
    title        = {Drought, deluge and declines: the impact of precipitation
        extremes on amphibians in a changing climate},
    journal      = {Biology},
    year         = {2013},
    volume       = {2},
    number       = {1},
    pages        = {399--418},
    urldate      = {2013-11-04},
    url          = {http://www.mdpi.com/2079-7737/2/1/399},
    doi          = {10.3390/biology2010399},
}
@article{Bohan1928,
    author = { ボハン, デ},
    title = { 過去及び現在に於ける英国と会 },
    journal = { 日本時報 },
    year = { 1928 },
    volume = { 17 },
    pages = { 5-9 },
    edition = { 9 },
    hyphenation = { japanese },
    language = { japanese }
}

@article{Dubrovin1906,
    author = { Дубровин, А. И },
    title = { Открытое письмо Председателя Главного Совета Союза Русского Народа Санкт-Петербургскому Антонию, Первенствующему члену Священного Синода },
    journal = { Вече },
    year = { 1906 },
    volume = {  },
    edition = { 97 },
    month = { 7 дек. 1906 },
    pages = { 1-3 },
    hyphenation = { russian },
    language = { russian }
}

@book{zong2013,
    author       = {宗成庆},
    key          = {Zong Cheng Qing},
    title        = {统计自然语言处理},
    address      = {北京},
    publisher    = {清华大学出版社},
    year         = {2013}
}
@inproceedings{kalchbrenner-blunsom-2013-recurrent,
    title = "Recurrent Continuous Translation Models",
    author = "Kalchbrenner, Nal  and
      Blunsom, Phil",
    booktitle = "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing",
    month = oct,
    year = "2013",
    address = "Seattle, Washington, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D13-1176",
    pages = "1700--1709",
}
@article{fukushima1980neocognitron,
  title={Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position},
  author={Fukushima, Kunihiko},
  journal={Biological cybernetics},
  volume={36},
  number={4},
  pages={193--202},
  year={1980},
  publisher={Springer}
}
@article{DBLP:journals/pieee/LeCunBBH98,
  author    = {Yann LeCun and
               L{\'{e}}on Bottou and
               Yoshua Bengio and
               Patrick Haffner},
  title     = {Gradient-based learning applied to document recognition},
  journal   = {Proc. {IEEE}},
  volume    = {86},
  number    = {11},
  pages     = {2278--2324},
  year      = {1998},
  url       = {https://doi.org/10.1109/5.726791},
  doi       = {10.1109/5.726791},
  timestamp = {Wed, 16 Mar 2022 23:54:13 +0100},
  biburl    = {https://dblp.org/rec/journals/pieee/LeCunBBH98.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{lecun1989handwritten,
  title={Handwritten digit recognition with a back-propagation network},
  author={LeCun, Yann and Boser, Bernhard and Denker, John and Henderson, Donnie and Howard, Richard and Hubbard, Wayne and Jackel, Lawrence},
  journal={Advances in neural information processing systems},
  volume={2},
  year={1989}
}
@inproceedings{DBLP:conf/nips/KrizhevskySH12,
  author    = {Alex Krizhevsky and
               Ilya Sutskever and
               Geoffrey E. Hinton},
  editor    = {Peter L. Bartlett and
               Fernando C. N. Pereira and
               Christopher J. C. Burges and
               L{\'{e}}on Bottou and
               Kilian Q. Weinberger},
  title     = {ImageNet Classification with Deep Convolutional Neural Networks},
  booktitle = {Advances in Neural Information Processing Systems 25: 26th Annual
               Conference on Neural Information Processing Systems 2012. Proceedings
               of a meeting held December 3-6, 2012, Lake Tahoe, Nevada, United States},
  pages     = {1106--1114},
  year      = {2012},
  url       = {https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html},
  timestamp = {Mon, 16 May 2022 15:41:51 +0200},
  biburl    = {https://dblp.org/rec/conf/nips/KrizhevskySH12.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{DBLP:journals/corr/SimonyanZ14a,
  author    = {Karen Simonyan and
               Andrew Zisserman},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
  booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015,
               San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year      = {2015},
  url       = {http://arxiv.org/abs/1409.1556},
  timestamp = {Wed, 17 Jul 2019 10:40:54 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/SimonyanZ14a.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/corr/SzegedyLJSRAEVR14,
  author    = {Christian Szegedy and
               Wei Liu and
               Yangqing Jia and
               Pierre Sermanet and
               Scott E. Reed and
               Dragomir Anguelov and
               Dumitru Erhan and
               Vincent Vanhoucke and
               Andrew Rabinovich},
  title     = {Going Deeper with Convolutions},
  journal   = {CoRR},
  volume    = {abs/1409.4842},
  year      = {2014},
  url       = {http://arxiv.org/abs/1409.4842},
  eprinttype = {arXiv},
  eprint    = {1409.4842},
  timestamp = {Mon, 13 Aug 2018 16:48:52 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/SzegedyLJSRAEVR14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{DBLP:conf/cvpr/HuangLMW17,
  author    = {Gao Huang and
               Zhuang Liu and
               Laurens van der Maaten and
               Kilian Q. Weinberger},
  title     = {Densely Connected Convolutional Networks},
  booktitle = {2017 {IEEE} Conference on Computer Vision and Pattern Recognition,
               {CVPR} 2017, Honolulu, HI, USA, July 21-26, 2017},
  pages     = {2261--2269},
  publisher = {{IEEE} Computer Society},
  year      = {2017},
  url       = {https://doi.org/10.1109/CVPR.2017.243},
  doi       = {10.1109/CVPR.2017.243},
  timestamp = {Wed, 16 Oct 2019 14:14:50 +0200},
  biburl    = {https://dblp.org/rec/conf/cvpr/HuangLMW17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
% R-CNN
@inproceedings{DBLP:conf/cvpr/GirshickDDM14,
  author    = {Ross B. Girshick and
               Jeff Donahue and
               Trevor Darrell and
               Jitendra Malik},
  title     = {Rich Feature Hierarchies for Accurate Object Detection and Semantic
               Segmentation},
  booktitle = {2014 {IEEE} Conference on Computer Vision and Pattern Recognition,
               {CVPR} 2014, Columbus, OH, USA, June 23-28, 2014},
  pages     = {580--587},
  publisher = {{IEEE} Computer Society},
  year      = {2014},
  url       = {https://doi.org/10.1109/CVPR.2014.81},
  doi       = {10.1109/CVPR.2014.81},
  timestamp = {Wed, 16 Oct 2019 14:14:50 +0200},
  biburl    = {https://dblp.org/rec/conf/cvpr/GirshickDDM14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
% Fast R-CNN
@inproceedings{DBLP:conf/iccv/Girshick15,
  author    = {Ross B. Girshick},
  title     = {Fast {R-CNN}},
  booktitle = {2015 {IEEE} International Conference on Computer Vision, {ICCV} 2015,
               Santiago, Chile, December 7-13, 2015},
  pages     = {1440--1448},
  publisher = {{IEEE} Computer Society},
  year      = {2015},
  url       = {https://doi.org/10.1109/ICCV.2015.169},
  doi       = {10.1109/ICCV.2015.169},
  timestamp = {Wed, 16 Oct 2019 14:14:51 +0200},
  biburl    = {https://dblp.org/rec/conf/iccv/Girshick15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
% Faster R-CNN
@inproceedings{DBLP:conf/nips/RenHGS15,
  author    = {Shaoqing Ren and
               Kaiming He and
               Ross B. Girshick and
               Jian Sun},
  editor    = {Corinna Cortes and
               Neil D. Lawrence and
               Daniel D. Lee and
               Masashi Sugiyama and
               Roman Garnett},
  title     = {Faster {R-CNN:} Towards Real-Time Object Detection with Region Proposal
               Networks},
  booktitle = {Advances in Neural Information Processing Systems 28: Annual Conference
               on Neural Information Processing Systems 2015, December 7-12, 2015,
               Montreal, Quebec, Canada},
  pages     = {91--99},
  year      = {2015},
  url       = {https://proceedings.neurips.cc/paper/2015/hash/14bfa6bb14875e45bba028a21ed38046-Abstract.html},
  timestamp = {Mon, 16 May 2022 15:41:51 +0200},
  biburl    = {https://dblp.org/rec/conf/nips/RenHGS15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
% YOLO
@inproceedings{DBLP:conf/cvpr/RedmonDGF16,
  author    = {Joseph Redmon and
               Santosh Kumar Divvala and
               Ross B. Girshick and
               Ali Farhadi},
  title     = {You Only Look Once: Unified, Real-Time Object Detection},
  booktitle = {2016 {IEEE} Conference on Computer Vision and Pattern Recognition,
               {CVPR} 2016, Las Vegas, NV, USA, June 27-30, 2016},
  pages     = {779--788},
  publisher = {{IEEE} Computer Society},
  year      = {2016},
  url       = {https://doi.org/10.1109/CVPR.2016.91},
  doi       = {10.1109/CVPR.2016.91},
  timestamp = {Wed, 16 Oct 2019 14:14:50 +0200},
  biburl    = {https://dblp.org/rec/conf/cvpr/RedmonDGF16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
% YOLOv2
@inproceedings{DBLP:conf/cvpr/RedmonF17,
  author    = {Joseph Redmon and
               Ali Farhadi},
  title     = {{YOLO9000:} Better, Faster, Stronger},
  booktitle = {2017 {IEEE} Conference on Computer Vision and Pattern Recognition,
               {CVPR} 2017, Honolulu, HI, USA, July 21-26, 2017},
  pages     = {6517--6525},
  publisher = {{IEEE} Computer Society},
  year      = {2017},
  url       = {https://doi.org/10.1109/CVPR.2017.690},
  doi       = {10.1109/CVPR.2017.690},
  timestamp = {Wed, 16 Oct 2019 14:14:50 +0200},
  biburl    = {https://dblp.org/rec/conf/cvpr/RedmonF17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
% YOLOv3
@article{DBLP:journals/corr/abs-1804-02767,
  author    = {Joseph Redmon and
               Ali Farhadi},
  title     = {YOLOv3: An Incremental Improvement},
  journal   = {CoRR},
  volume    = {abs/1804.02767},
  year      = {2018},
  url       = {http://arxiv.org/abs/1804.02767},
  eprinttype = {arXiv},
  eprint    = {1804.02767},
  timestamp = {Mon, 13 Aug 2018 16:48:24 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1804-02767.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
% YOLOv4
@article{DBLP:journals/corr/abs-2004-10934,
  author    = {Alexey Bochkovskiy and
               Chien{-}Yao Wang and
               Hong{-}Yuan Mark Liao},
  title     = {YOLOv4: Optimal Speed and Accuracy of Object Detection},
  journal   = {CoRR},
  volume    = {abs/2004.10934},
  year      = {2020},
  url       = {https://arxiv.org/abs/2004.10934},
  eprinttype = {arXiv},
  eprint    = {2004.10934},
  timestamp = {Tue, 28 Apr 2020 16:10:02 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2004-10934.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
% YOLOv5
@article{DBLP:journals/corr/abs-2107-08430,
  author    = {Zheng Ge and
               Songtao Liu and
               Feng Wang and
               Zeming Li and
               Jian Sun},
  title     = {{YOLOX:} Exceeding {YOLO} Series in 2021},
  journal   = {CoRR},
  volume    = {abs/2107.08430},
  year      = {2021},
  url       = {https://arxiv.org/abs/2107.08430},
  eprinttype = {arXiv},
  eprint    = {2107.08430},
  timestamp = {Tue, 05 Apr 2022 14:09:44 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2107-08430.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
%1
@article{1_DBLP:journals/corr/SutskeverVL14,
  author    = {Ilya Sutskever and
               Oriol Vinyals and
               Quoc V. Le},
  title     = {Sequence to Sequence Learning with Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1409.3215},
  year      = {2014},
  url       = {http://arxiv.org/abs/1409.3215},
  eprinttype = {arXiv},
  eprint    = {1409.3215},
  timestamp = {Mon, 13 Aug 2018 16:48:06 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/SutskeverVL14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
%2
@inproceedings{2_cho-etal-2014-learning,
    title = "Learning Phrase Representations using {RNN} Encoder{--}Decoder for Statistical Machine Translation",
    author = {Cho, Kyunghyun  and
      van Merri{\"e}nboer, Bart  and
      Gulcehre, Caglar  and
      Bahdanau, Dzmitry  and
      Bougares, Fethi  and
      Schwenk, Holger  and
      Bengio, Yoshua},
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D14-1179",
    doi = "10.3115/v1/D14-1179",
    pages = "1724--1734",
}
%3
@inproceedings{3_DBLP:journals/corr/BahdanauCB14,
  author    = {Dzmitry Bahdanau and
               Kyunghyun Cho and
               Yoshua Bengio},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Neural Machine Translation by Jointly Learning to Align and Translate},
  booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015,
               San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year      = {2015},
  url       = {http://arxiv.org/abs/1409.0473},
  timestamp = {Wed, 17 Jul 2019 10:40:54 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/BahdanauCB14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
%4
@inproceedings{4_luong-etal-2015-effective,
    title = "Effective Approaches to Attention-based Neural Machine Translation",
    author = "Luong, Thang  and
      Pham, Hieu  and
      Manning, Christopher D.",
    booktitle = "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2015",
    address = "Lisbon, Portugal",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D15-1166",
    doi = "10.18653/v1/D15-1166",
    pages = "1412--1421",
}
%5
@article{5_DBLP:journals/corr/VaswaniSPUJGKP17,
  author    = {Ashish Vaswani and
               Noam Shazeer and
               Niki Parmar and
               Jakob Uszkoreit and
               Llion Jones and
               Aidan N. Gomez and
               Lukasz Kaiser and
               Illia Polosukhin},
  title     = {Attention Is All You Need},
  journal   = {CoRR},
  volume    = {abs/1706.03762},
  year      = {2017},
  url       = {http://arxiv.org/abs/1706.03762},
  eprinttype = {arXiv},
  eprint    = {1706.03762},
  timestamp = {Sat, 23 Jan 2021 01:20:40 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/VaswaniSPUJGKP17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
%6
@article{6_DBLP:journals/corr/GehringAGYD17,
  author    = {Jonas Gehring and
               Michael Auli and
               David Grangier and
               Denis Yarats and
               Yann N. Dauphin},
  title     = {Convolutional Sequence to Sequence Learning},
  journal   = {CoRR},
  volume    = {abs/1705.03122},
  year      = {2017},
  url       = {http://arxiv.org/abs/1705.03122},
  eprinttype = {arXiv},
  eprint    = {1705.03122},
  timestamp = {Mon, 13 Aug 2018 16:48:03 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/GehringAGYD17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
%7
@inproceedings{7_DBLP:conf/naacl/DevlinCLT19,
  author    = {Jacob Devlin and
               Ming{-}Wei Chang and
               Kenton Lee and
               Kristina Toutanova},
  editor    = {Jill Burstein and
               Christy Doran and
               Thamar Solorio},
  title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language
               Understanding},
  booktitle = {Proceedings of the 2019 Conference of the North American Chapter of
               the Association for Computational Linguistics: Human Language Technologies,
               {NAACL-HLT} 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long
               and Short Papers)},
  pages     = {4171--4186},
  publisher = {Association for Computational Linguistics},
  year      = {2019},
  url       = {https://doi.org/10.18653/v1/n19-1423},
  doi       = {10.18653/v1/n19-1423},
  timestamp = {Mon, 26 Sep 2022 12:21:55 +0200},
  biburl    = {https://dblp.org/rec/conf/naacl/DevlinCLT19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
%8
@article{8_DBLP:journals/corr/abs-1803-02155,
  author    = {Peter Shaw and
               Jakob Uszkoreit and
               Ashish Vaswani},
  title     = {Self-Attention with Relative Position Representations},
  journal   = {CoRR},
  volume    = {abs/1803.02155},
  year      = {2018},
  url       = {http://arxiv.org/abs/1803.02155},
  eprinttype = {arXiv},
  eprint    = {1803.02155},
  timestamp = {Mon, 13 Aug 2018 16:46:37 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1803-02155.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
%9
@article{9_DBLP:journals/corr/abs-1901-02860,
  author    = {Zihang Dai and
               Zhilin Yang and
               Yiming Yang and
               Jaime G. Carbonell and
               Quoc V. Le and
               Ruslan Salakhutdinov},
  title     = {Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context},
  journal   = {CoRR},
  volume    = {abs/1901.02860},
  year      = {2019},
  url       = {http://arxiv.org/abs/1901.02860},
  eprinttype = {arXiv},
  eprint    = {1901.02860},
  timestamp = {Fri, 01 Feb 2019 13:39:59 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1901-02860.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
%10
@article{10_DBLP:journals/corr/abs-1802-05751,
  author    = {Niki Parmar and
               Ashish Vaswani and
               Jakob Uszkoreit and
               Lukasz Kaiser and
               Noam Shazeer and
               Alexander Ku},
  title     = {Image Transformer},
  journal   = {CoRR},
  volume    = {abs/1802.05751},
  year      = {2018},
  url       = {http://arxiv.org/abs/1802.05751},
  eprinttype = {arXiv},
  eprint    = {1802.05751},
  timestamp = {Mon, 13 Aug 2018 16:48:00 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1802-05751.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
%11
@inproceedings{11_DBLP:conf/iclr/DosovitskiyB0WZ21,
  author    = {Alexey Dosovitskiy and
               Lucas Beyer and
               Alexander Kolesnikov and
               Dirk Weissenborn and
               Xiaohua Zhai and
               Thomas Unterthiner and
               Mostafa Dehghani and
               Matthias Minderer and
               Georg Heigold and
               Sylvain Gelly and
               Jakob Uszkoreit and
               Neil Houlsby},
  title     = {An Image is Worth 16x16 Words: Transformers for Image Recognition
               at Scale},
  booktitle = {9th International Conference on Learning Representations, {ICLR} 2021,
               Virtual Event, Austria, May 3-7, 2021},
  publisher = {OpenReview.net},
  year      = {2021},
  url       = {https://openreview.net/forum?id=YicbFdNTTy},
  timestamp = {Wed, 23 Jun 2021 17:36:39 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/DosovitskiyB0WZ21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
%12
@inproceedings{12_DBLP:conf/iccv/LiuL00W0LG21,
  author    = {Ze Liu and
               Yutong Lin and
               Yue Cao and
               Han Hu and
               Yixuan Wei and
               Zheng Zhang and
               Stephen Lin and
               Baining Guo},
  title     = {Swin Transformer: Hierarchical Vision Transformer using Shifted Windows},
  booktitle = {2021 {IEEE/CVF} International Conference on Computer Vision, {ICCV}
               2021, Montreal, QC, Canada, October 10-17, 2021},
  pages     = {9992--10002},
  publisher = {{IEEE}},
  year      = {2021},
  url       = {https://doi.org/10.1109/ICCV48922.2021.00986},
  doi       = {10.1109/ICCV48922.2021.00986},
  timestamp = {Thu, 19 May 2022 16:00:58 +0200},
  biburl    = {https://dblp.org/rec/conf/iccv/LiuL00W0LG21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
%13
@inproceedings{13_DBLP:conf/iccv/0007CWYSJTFY21,
  author    = {Li Yuan and
               Yunpeng Chen and
               Tao Wang and
               Weihao Yu and
               Yujun Shi and
               Zihang Jiang and
               Francis E. H. Tay and
               Jiashi Feng and
               Shuicheng Yan},
  title     = {Tokens-to-Token ViT: Training Vision Transformers from Scratch on
               ImageNet},
  booktitle = {2021 {IEEE/CVF} International Conference on Computer Vision, {ICCV}
               2021, Montreal, QC, Canada, October 10-17, 2021},
  pages     = {538--547},
  publisher = {{IEEE}},
  year      = {2021},
  url       = {https://doi.org/10.1109/ICCV48922.2021.00060},
  doi       = {10.1109/ICCV48922.2021.00060},
  timestamp = {Mon, 04 Apr 2022 16:15:33 +0200},
  biburl    = {https://dblp.org/rec/conf/iccv/0007CWYSJTFY21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
%14
@inproceedings{14_DBLP:conf/nips/LuBPL19,
  author    = {Jiasen Lu and
               Dhruv Batra and
               Devi Parikh and
               Stefan Lee},
  editor    = {Hanna M. Wallach and
               Hugo Larochelle and
               Alina Beygelzimer and
               Florence d'Alch{\'{e}}{-}Buc and
               Emily B. Fox and
               Roman Garnett},
  title     = {ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations
               for Vision-and-Language Tasks},
  booktitle = {Advances in Neural Information Processing Systems 32: Annual Conference
               on Neural Information Processing Systems 2019, NeurIPS 2019, December
               8-14, 2019, Vancouver, BC, Canada},
  pages     = {13--23},
  year      = {2019},
  url       = {https://proceedings.neurips.cc/paper/2019/hash/c74d97b01eae257e44aa9d5bade97baf-Abstract.html},
  timestamp = {Mon, 16 May 2022 15:41:51 +0200},
  biburl    = {https://dblp.org/rec/conf/nips/LuBPL19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
%15
@inproceedings{15_DBLP:conf/eccv/ChenLYK0G0020,
  author    = {Yen{-}Chun Chen and
               Linjie Li and
               Licheng Yu and
               Ahmed El Kholy and
               Faisal Ahmed and
               Zhe Gan and
               Yu Cheng and
               Jingjing Liu},
  editor    = {Andrea Vedaldi and
               Horst Bischof and
               Thomas Brox and
               Jan{-}Michael Frahm},
  title     = {{UNITER:} UNiversal Image-TExt Representation Learning},
  booktitle = {Computer Vision - {ECCV} 2020 - 16th European Conference, Glasgow,
               UK, August 23-28, 2020, Proceedings, Part {XXX}},
  series    = {Lecture Notes in Computer Science},
  volume    = {12375},
  pages     = {104--120},
  publisher = {Springer},
  year      = {2020},
  url       = {https://doi.org/10.1007/978-3-030-58577-8\_7},
  doi       = {10.1007/978-3-030-58577-8\_7},
  timestamp = {Sun, 02 Oct 2022 15:59:30 +0200},
  biburl    = {https://dblp.org/rec/conf/eccv/ChenLYK0G0020.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
%16
@article{16_DBLP:journals/corr/abs-2004-00849,
  author    = {Zhicheng Huang and
               Zhaoyang Zeng and
               Bei Liu and
               Dongmei Fu and
               Jianlong Fu},
  title     = {Pixel-BERT: Aligning Image Pixels with Text by Deep Multi-Modal Transformers},
  journal   = {CoRR},
  volume    = {abs/2004.00849},
  year      = {2020},
  url       = {https://arxiv.org/abs/2004.00849},
  eprinttype = {arXiv},
  eprint    = {2004.00849},
  timestamp = {Fri, 13 Aug 2021 14:56:28 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2004-00849.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
%17
@inproceedings{17_DBLP:conf/icml/KimSK21,
  author    = {Wonjae Kim and
               Bokyung Son and
               Ildoo Kim},
  editor    = {Marina Meila and
               Tong Zhang},
  title     = {ViLT: Vision-and-Language Transformer Without Convolution or Region
               Supervision},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning,
               {ICML} 2021, 18-24 July 2021, Virtual Event},
  series    = {Proceedings of Machine Learning Research},
  volume    = {139},
  pages     = {5583--5594},
  publisher = {{PMLR}},
  year      = {2021},
  url       = {http://proceedings.mlr.press/v139/kim21k.html},
  timestamp = {Wed, 25 Aug 2021 17:11:17 +0200},
  biburl    = {https://dblp.org/rec/conf/icml/KimSK21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
%18
@inproceedings{18_DBLP:conf/emnlp/CalixtoL17,
  author    = {Iacer Calixto and
               Qun Liu},
  editor    = {Martha Palmer and
               Rebecca Hwa and
               Sebastian Riedel},
  title     = {Incorporating Global Visual Features into Attention-based Neural Machine
               Translation},
  booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural
               Language Processing, {EMNLP} 2017, Copenhagen, Denmark, September
               9-11, 2017},
  pages     = {992--1003},
  publisher = {Association for Computational Linguistics},
  year      = {2017},
  url       = {https://doi.org/10.18653/v1/d17-1105},
  doi       = {10.18653/v1/d17-1105},
  timestamp = {Thu, 02 Sep 2021 09:00:27 +0200},
  biburl    = {https://dblp.org/rec/conf/emnlp/CalixtoL17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
%19
@inproceedings{19_DBLP:conf/acl/CalixtoRA19,
  author    = {Iacer Calixto and
               Miguel Rios and
               Wilker Aziz},
  editor    = {Anna Korhonen and
               David R. Traum and
               Llu{\'{\i}}s M{\`{a}}rquez},
  title     = {Latent Variable Model for Multi-modal Translation},
  booktitle = {Proceedings of the 57th Conference of the Association for Computational
               Linguistics, {ACL} 2019, Florence, Italy, July 28- August 2, 2019,
               Volume 1: Long Papers},
  pages     = {6392--6405},
  publisher = {Association for Computational Linguistics},
  year      = {2019},
  url       = {https://doi.org/10.18653/v1/p19-1642},
  doi       = {10.18653/v1/p19-1642},
  timestamp = {Sun, 02 Oct 2022 15:53:42 +0200},
  biburl    = {https://dblp.org/rec/conf/acl/CalixtoRA19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
%20
@inproceedings{20_wu-etal-2021-good,
    title = "Good for Misconceived Reasons: An Empirical Revisiting on the Need for Visual Context in Multimodal Machine Translation",
    author = "Wu, Zhiyong  and
      Kong, Lingpeng  and
      Bi, Wei  and
      Li, Xiang  and
      Kao, Ben",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.480",
    doi = "10.18653/v1/2021.acl-long.480",
    pages = "6153--6166",
    abstract = "A neural multimodal machine translation (MMT) system is one that aims to perform better translation by extending conventional text-only translation models with multimodal information. Many recent studies report improvements when equipping their models with the multimodal module, despite the controversy of whether such improvements indeed come from the multimodal part. We revisit the contribution of multimodal information in MMT by devising two interpretable MMT models. To our surprise, although our models replicate similar gains as recently developed multimodal-integrated systems achieved, our models learn to ignore the multimodal information. Upon further investigation, we discover that the improvements achieved by the multimodal models over text-only counterparts are in fact results of the regularization effect. We report empirical findings that highlight the importance of MMT models{'} interpretability, and discuss how our findings will benefit future research.",
}

%21
@inproceedings{21_dutta-chowdhury-elliott-2019-understanding,
    title = "Understanding the Effect of Textual Adversaries in Multimodal Machine Translation",
    author = "Dutta Chowdhury, Koel  and
      Elliott, Desmond",
    booktitle = "Proceedings of the Beyond Vision and LANguage: inTEgrating Real-world kNowledge (LANTERN)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-6406",
    doi = "10.18653/v1/D19-6406",
    pages = "35--40",
    abstract = "It is assumed that multimodal machine translation systems are better than text-only systems at translating phrases that have a direct correspondence in the image. This assumption has been challenged in experiments demonstrating that state-of-the-art multimodal systems perform equally well in the presence of randomly selected images, but, more recently, it has been shown that masking entities from the source language sentence during training can help to overcome this problem. In this paper, we conduct experiments with both visual and textual adversaries in order to understand the role of incorrect textual inputs to such systems. Our results show that when the source language sentence contains mistakes, multimodal translation systems do not leverage the additional visual signal to produce the correct translation. We also find that the degradation of translation performance caused by textual adversaries is significantly higher than by visual adversaries.",
}
%22
@inproceedings{22_li-etal-2021-vision,
    title = "Vision Matters When It Should: Sanity Checking Multimodal Machine Translation Models",
    author = "Li, Jiaoda  and
      Ataman, Duygu  and
      Sennrich, Rico",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.673",
    doi = "10.18653/v1/2021.emnlp-main.673",
    pages = "8556--8562",
    abstract = "Multimodal machine translation (MMT) systems have been shown to outperform their text-only neural machine translation (NMT) counterparts when visual context is available. However, recent studies have also shown that the performance of MMT models is only marginally impacted when the associated image is replaced with an unrelated image or noise, which suggests that the visual context might not be exploited by the model at all. We hypothesize that this might be caused by the nature of the commonly used evaluation benchmark, also known as Multi30K, where the translations of image captions were prepared without actually showing the images to human translators. In this paper, we present a qualitative study that examines the role of datasets in stimulating the leverage of visual modality and we propose methods to highlight the importance of visual signals in the datasets which demonstrate improvements in reliance of models on the source images. Our findings suggest the research on effective MMT architectures is currently impaired by the lack of suitable datasets and careful consideration must be taken in creation of future MMT datasets, for which we also provide useful insights.",
}
%23
@inproceedings{23_elliott-2018-adversarial,
    title = "Adversarial Evaluation of Multimodal Machine Translation",
    author = "Elliott, Desmond",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1329",
    doi = "10.18653/v1/D18-1329",
    pages = "2974--2978",
    abstract = "The promise of combining language and vision in multimodal machine translation is that systems will produce better translations by leveraging the image data. However, the evidence surrounding whether the images are useful is unconvincing due to inconsistencies between text-similarity metrics and human judgements. We present an adversarial evaluation to directly examine the utility of the image data in this task. Our evaluation tests whether systems perform better when paired with congruent images or incongruent images. This evaluation shows that only one out of three publicly available systems is sensitive to this perturbation of the data. We recommend that multimodal translation systems should be able to pass this sanity check in the future.",
}
%24
@inproceedings{24_DBLP:conf/iccv/YangGWHYL19,
  author    = {Zhengyuan Yang and
               Boqing Gong and
               Liwei Wang and
               Wenbing Huang and
               Dong Yu and
               Jiebo Luo},
  title     = {A Fast and Accurate One-Stage Approach to Visual Grounding},
  booktitle = {2019 {IEEE/CVF} International Conference on Computer Vision, {ICCV}
               2019, Seoul, Korea (South), October 27 - November 2, 2019},
  pages     = {4682--4692},
  publisher = {{IEEE}},
  year      = {2019},
  url       = {https://doi.org/10.1109/ICCV.2019.00478},
  doi       = {10.1109/ICCV.2019.00478},
  timestamp = {Wed, 07 Dec 2022 23:07:05 +0100},
  biburl    = {https://dblp.org/rec/conf/iccv/YangGWHYL19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
%25
@inproceedings{25_DBLP:conf/naacl/DevlinCLT19,
  author    = {Jacob Devlin and
               Ming{-}Wei Chang and
               Kenton Lee and
               Kristina Toutanova},
  editor    = {Jill Burstein and
               Christy Doran and
               Thamar Solorio},
  title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language
               Understanding},
  booktitle = {Proceedings of the 2019 Conference of the North American Chapter of
               the Association for Computational Linguistics: Human Language Technologies,
               {NAACL-HLT} 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long
               and Short Papers)},
  pages     = {4171--4186},
  publisher = {Association for Computational Linguistics},
  year      = {2019},
  url       = {https://doi.org/10.18653/v1/n19-1423},
  doi       = {10.18653/v1/n19-1423},
  timestamp = {Mon, 26 Sep 2022 12:21:55 +0200},
  biburl    = {https://dblp.org/rec/conf/naacl/DevlinCLT19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
%26
@article{26_DBLP:journals/corr/abs-1804-02767,
  author    = {Joseph Redmon and
               Ali Farhadi},
  title     = {YOLOv3: An Incremental Improvement},
  journal   = {CoRR},
  volume    = {abs/1804.02767},
  year      = {2018},
  url       = {http://arxiv.org/abs/1804.02767},
  eprinttype = {arXiv},
  eprint    = {1804.02767},
  timestamp = {Mon, 13 Aug 2018 16:48:24 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1804-02767.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
%27
@inproceedings{27_sennrich-etal-2016-neural,
    title = "Neural Machine Translation of Rare Words with Subword Units",
    author = "Sennrich, Rico  and
      Haddow, Barry  and
      Birch, Alexandra",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P16-1162",
    doi = "10.18653/v1/P16-1162",
    pages = "1715--1725",
}
%28
@article{28_DBLP:journals/corr/WuSCLNMKCGMKSJL16,
  author    = {Yonghui Wu and
               Mike Schuster and
               Zhifeng Chen and
               Quoc V. Le and
               Mohammad Norouzi and
               Wolfgang Macherey and
               Maxim Krikun and
               Yuan Cao and
               Qin Gao and
               Klaus Macherey and
               Jeff Klingner and
               Apurva Shah and
               Melvin Johnson and
               Xiaobing Liu and
               Lukasz Kaiser and
               Stephan Gouws and
               Yoshikiyo Kato and
               Taku Kudo and
               Hideto Kazawa and
               Keith Stevens and
               George Kurian and
               Nishant Patil and
               Wei Wang and
               Cliff Young and
               Jason Smith and
               Jason Riesa and
               Alex Rudnick and
               Oriol Vinyals and
               Greg Corrado and
               Macduff Hughes and
               Jeffrey Dean},
  title     = {Google's Neural Machine Translation System: Bridging the Gap between
               Human and Machine Translation},
  journal   = {CoRR},
  volume    = {abs/1609.08144},
  year      = {2016},
  url       = {http://arxiv.org/abs/1609.08144},
  eprinttype = {arXiv},
  eprint    = {1609.08144},
  timestamp = {Thu, 14 Jan 2021 12:12:19 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/WuSCLNMKCGMKSJL16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
%29
@inproceedings{29_DBLP:conf/iccv/PlummerWCCHL15,
  author    = {Bryan A. Plummer and
               Liwei Wang and
               Chris M. Cervantes and
               Juan C. Caicedo and
               Julia Hockenmaier and
               Svetlana Lazebnik},
  title     = {Flickr30k Entities: Collecting Region-to-Phrase Correspondences for
               Richer Image-to-Sentence Models},
  booktitle = {2015 {IEEE} International Conference on Computer Vision, {ICCV} 2015,
               Santiago, Chile, December 7-13, 2015},
  pages     = {2641--2649},
  publisher = {{IEEE} Computer Society},
  year      = {2015},
  url       = {https://doi.org/10.1109/ICCV.2015.303},
  doi       = {10.1109/ICCV.2015.303},
  timestamp = {Wed, 16 Oct 2019 14:14:51 +0200},
  biburl    = {https://dblp.org/rec/conf/iccv/PlummerWCCHL15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
%30
@article{30_DBLP:journals/ijcv/PlummerWCCHL17,
  author    = {Bryan A. Plummer and
               Liwei Wang and
               Chris M. Cervantes and
               Juan C. Caicedo and
               Julia Hockenmaier and
               Svetlana Lazebnik},
  title     = {Flickr30k Entities: Collecting Region-to-Phrase Correspondences for
               Richer Image-to-Sentence Models},
  journal   = {Int. J. Comput. Vis.},
  volume    = {123},
  number    = {1},
  pages     = {74--93},
  year      = {2017},
  url       = {https://doi.org/10.1007/s11263-016-0965-7},
  doi       = {10.1007/s11263-016-0965-7},
  timestamp = {Fri, 13 Mar 2020 10:59:37 +0100},
  biburl    = {https://dblp.org/rec/journals/ijcv/PlummerWCCHL17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
%31
@article{31_DBLP:journals/ijcv/RussakovskyDSKS15,
  author    = {Olga Russakovsky and
               Jia Deng and
               Hao Su and
               Jonathan Krause and
               Sanjeev Satheesh and
               Sean Ma and
               Zhiheng Huang and
               Andrej Karpathy and
               Aditya Khosla and
               Michael S. Bernstein and
               Alexander C. Berg and
               Li Fei{-}Fei},
  title     = {ImageNet Large Scale Visual Recognition Challenge},
  journal   = {Int. J. Comput. Vis.},
  volume    = {115},
  number    = {3},
  pages     = {211--252},
  year      = {2015},
  url       = {https://doi.org/10.1007/s11263-015-0816-y},
  doi       = {10.1007/s11263-015-0816-y},
  timestamp = {Tue, 10 Jan 2023 08:57:16 +0100},
  biburl    = {https://dblp.org/rec/journals/ijcv/RussakovskyDSKS15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
%32
@inproceedings{32_DBLP:conf/cvpr/HeZRS16,
  author    = {Kaiming He and
               Xiangyu Zhang and
               Shaoqing Ren and
               Jian Sun},
  title     = {Deep Residual Learning for Image Recognition},
  booktitle = {2016 {IEEE} Conference on Computer Vision and Pattern Recognition,
               {CVPR} 2016, Las Vegas, NV, USA, June 27-30, 2016},
  pages     = {770--778},
  publisher = {{IEEE} Computer Society},
  year      = {2016},
  url       = {https://doi.org/10.1109/CVPR.2016.90},
  doi       = {10.1109/CVPR.2016.90},
  timestamp = {Wed, 25 Jan 2023 11:01:16 +0100},
  biburl    = {https://dblp.org/rec/conf/cvpr/HeZRS16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
%33
@inproceedings{33_yin-etal-2020-novel,
    title = "A Novel Graph-based Multi-modal Fusion Encoder for Neural Machine Translation",
    author = "Yin, Yongjing  and
      Meng, Fandong  and
      Su, Jinsong  and
      Zhou, Chulun  and
      Yang, Zhengyuan  and
      Zhou, Jie  and
      Luo, Jiebo",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.273",
    doi = "10.18653/v1/2020.acl-main.273",
    pages = "3025--3035",
    abstract = "Multi-modal neural machine translation (NMT) aims to translate source sentences into a target language paired with images. However, dominant multi-modal NMT models do not fully exploit fine-grained semantic correspondences between semantic units of different modalities, which have potential to refine multi-modal representation learning. To deal with this issue, in this paper, we propose a novel graph-based multi-modal fusion encoder for NMT. Specifically, we first represent the input sentence and image using a unified multi-modal graph, which captures various semantic relationships between multi-modal semantic units (words and visual objects). We then stack multiple graph-based multi-modal fusion layers that iteratively perform semantic interactions to learn node representations. Finally, these representations provide an attention-based context vector for the decoder. We evaluate our proposed encoder on the Multi30K datasets. Experimental results and in-depth analysis show the superiority of our multi-modal NMT model.",
}
%34
@inproceedings{34_DBLP:journals/corr/KingmaB14,
  author    = {Diederik P. Kingma and
               Jimmy Ba},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Adam: {A} Method for Stochastic Optimization},
  booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015,
               San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year      = {2015},
  url       = {http://arxiv.org/abs/1412.6980},
  timestamp = {Thu, 25 Jul 2019 14:25:37 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/KingmaB14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
%35
@inproceedings{35_huang-etal-2016-attention,
    title = "Attention-based Multimodal Neural Machine Translation",
    author = "Huang, Po-Yao  and
      Liu, Frederick  and
      Shiang, Sz-Rung  and
      Oh, Jean  and
      Dyer, Chris",
    booktitle = "Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W16-2360",
    doi = "10.18653/v1/W16-2360",
    pages = "639--645",
}
%36
@inproceedings{36_calixto-etal-2017-doubly,
    title = "Doubly-Attentive Decoder for Multi-modal Neural Machine Translation",
    author = "Calixto, Iacer  and
      Liu, Qun  and
      Campbell, Nick",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P17-1175",
    doi = "10.18653/v1/P17-1175",
    pages = "1913--1924",
    abstract = "We introduce a Multi-modal Neural Machine Translation model in which a doubly-attentive decoder naturally incorporates spatial visual features obtained using pre-trained convolutional neural networks, bridging the gap between image description and translation. Our decoder learns to attend to source-language words and parts of an image independently by means of two separate attention mechanisms as it generates words in the target language. We find that our model can efficiently exploit not just back-translated in-domain multi-modal data but also large general-domain text-only MT corpora. We also report state-of-the-art results on the Multi30k data set.",
}
%37
@inproceedings{37_elliott-kadar-2017-imagination,
    title = "Imagination Improves Multimodal Translation",
    author = "Elliott, Desmond  and
      K{\'a}d{\'a}r, {\'A}kos",
    booktitle = "Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = nov,
    year = "2017",
    address = "Taipei, Taiwan",
    publisher = "Asian Federation of Natural Language Processing",
    url = "https://aclanthology.org/I17-1014",
    pages = "130--141",
    abstract = "We decompose multimodal translation into two sub-tasks: learning to translate and learning visually grounded representations. In a multitask learning framework, translations are learned in an attention-based encoder-decoder, and grounded representations are learned through image representation prediction. Our approach improves translation performance compared to the state of the art on the Multi30K dataset. Furthermore, it is equally effective if we train the image prediction task on the external MS COCO dataset, and we find improvements if we train the translation model on the external News Commentary parallel text.",
}
%38
@inproceedings{38_calixto-etal-2019-latent,
    title = "Latent Variable Model for Multi-modal Translation",
    author = "Calixto, Iacer  and
      Rios, Miguel  and
      Aziz, Wilker",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1642",
    doi = "10.18653/v1/P19-1642",
    pages = "6392--6405",
    abstract = "In this work, we propose to model the interaction between visual and textual features for multi-modal neural machine translation (MMT) through a latent variable model. This latent variable can be seen as a multi-modal stochastic embedding of an image and its description in a foreign language. It is used in a target-language decoder and also to predict image features. Importantly, our model formulation utilises visual and textual inputs during training but does not require that images be available at test time. We show that our latent variable MMT formulation improves considerably over strong baselines, including a multi-task learning approach (Elliott and Kadar, 2017) and a conditional variational auto-encoder approach (Toyama et al., 2016). Finally, we show improvements due to (i) predicting image features in addition to only conditioning on them, (ii) imposing a constraint on the KL term to promote models with non-negligible mutual information between inputs and latent variable, and (iii) by training on additional target-language image descriptions (i.e. synthetic data).",
}

%39
@inproceedings{39_ive-etal-2019-distilling,
    title = "Distilling Translations with Visual Awareness",
    author = "Ive, Julia  and
      Madhyastha, Pranava  and
      Specia, Lucia",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1653",
    doi = "10.18653/v1/P19-1653",
    pages = "6525--6538",
    abstract = "Previous work on multimodal machine translation has shown that visual information is only needed in very specific cases, for example in the presence of ambiguous words where the textual context is not sufficient. As a consequence, models tend to learn to ignore this information. We propose a translate-and-refine approach to this problem where images are only used by a second stage decoder. This approach is trained jointly to generate a good first draft translation and to improve over this draft by (i) making better use of the target language textual context (both left and right-side contexts) and (ii) making use of visual context. This approach leads to the state of the art results. Additionally, we show that it has the ability to recover from erroneous or missing words in the source language.",
}
%40
@inproceedings{40_yao-wan-2020-multimodal,
    title = "Multimodal Transformer for Multimodal Machine Translation",
    author = "Yao, Shaowei  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.400",
    doi = "10.18653/v1/2020.acl-main.400",
    pages = "4346--4350",
    abstract = "Multimodal Machine Translation (MMT) aims to introduce information from other modality, generally static images, to improve the translation quality. Previous works propose various incorporation methods, but most of them do not consider the relative importance of multiple modalities. Equally treating all modalities may encode too much useless information from less important modalities. In this paper, we introduce the multimodal self-attention in Transformer to solve the issues above in MMT. The proposed method learns the representation of images based on the text, which avoids encoding irrelevant information in images. Experiments and visualization analysis demonstrate that our model benefits from visual information and substantially outperforms previous works and competitive baselines in terms of various metrics.",
}
%41
@article{41_DBLP:journals/corr/abs-2103-08862,
  author    = {Pengbo Liu and
               Hailong Cao and
               Tiejun Zhao},
  title     = {Gumbel-Attention for Multi-modal Machine Translation},
  journal   = {CoRR},
  volume    = {abs/2103.08862},
  year      = {2021},
  url       = {https://arxiv.org/abs/2103.08862},
  eprinttype = {arXiv},
  eprint    = {2103.08862},
  timestamp = {Tue, 23 Mar 2021 16:29:47 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2103-08862.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
%42
@inproceedings{42_papineni-etal-2002-bleu,
    title = "{B}leu: a Method for Automatic Evaluation of Machine Translation",
    author = "Papineni, Kishore  and
      Roukos, Salim  and
      Ward, Todd  and
      Zhu, Wei-Jing",
    booktitle = "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2002",
    address = "Philadelphia, Pennsylvania, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P02-1040",
    doi = "10.3115/1073083.1073135",
    pages = "311--318",
}
%43
@inproceedings{43_elliott-etal-2016-multi30k,
    title = "{M}ulti30{K}: Multilingual {E}nglish-{G}erman Image Descriptions",
    author = "Elliott, Desmond  and
      Frank, Stella  and
      Sima{'}an, Khalil  and
      Specia, Lucia",
    booktitle = "Proceedings of the 5th Workshop on Vision and Language",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W16-3210",
    doi = "10.18653/v1/W16-3210",
    pages = "70--74",
}
%44
@inproceedings{44_koehn-etal-2007-moses,
    title = "{M}oses: Open Source Toolkit for Statistical Machine Translation",
    author = "Koehn, Philipp  and
      Hoang, Hieu  and
      Birch, Alexandra  and
      Callison-Burch, Chris  and
      Federico, Marcello  and
      Bertoldi, Nicola  and
      Cowan, Brooke  and
      Shen, Wade  and
      Moran, Christine  and
      Zens, Richard  and
      Dyer, Chris  and
      Bojar, Ond{\v{r}}ej  and
      Constantin, Alexandra  and
      Herbst, Evan",
    booktitle = "Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions",
    month = jun,
    year = "2007",
    address = "Prague, Czech Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P07-2045",
    pages = "177--180",
}
%45
@inproceedings{45_dyer-etal-2013-simple,
    title = "A Simple, Fast, and Effective Reparameterization of {IBM} Model 2",
    author = "Dyer, Chris  and
      Chahuneau, Victor  and
      Smith, Noah A.",
    booktitle = "Proceedings of the 2013 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2013",
    address = "Atlanta, Georgia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N13-1073",
    pages = "644--648",
}
%46
@inproceedings{46_denkowski-lavie-2014-meteor,
    title = "Meteor Universal: Language Specific Translation Evaluation for Any Target Language",
    author = "Denkowski, Michael  and
      Lavie, Alon",
    booktitle = "Proceedings of the Ninth Workshop on Statistical Machine Translation",
    month = jun,
    year = "2014",
    address = "Baltimore, Maryland, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W14-3348",
    doi = "10.3115/v1/W14-3348",
    pages = "376--380",
}
%47
@inproceedings{47_DBLP:conf/wmt/LibovickyHM18,
  author    = {Jindrich Libovick{\'{y}} and
               Jindrich Helcl and
               David Marecek},
  editor    = {Ondrej Bojar and
               Rajen Chatterjee and
               Christian Federmann and
               Mark Fishel and
               Yvette Graham and
               Barry Haddow and
               Matthias Huck and
               Antonio Jimeno{-}Yepes and
               Philipp Koehn and
               Christof Monz and
               Matteo Negri and
               Aur{\'{e}}lie N{\'{e}}v{\'{e}}ol and
               Mariana L. Neves and
               Matt Post and
               Lucia Specia and
               Marco Turchi and
               Karin Verspoor},
  title     = {Input Combination Strategies for Multi-Source Transformer Decoder},
  booktitle = {Proceedings of the Third Conference on Machine Translation: Research
               Papers, {WMT} 2018, Belgium, Brussels, October 31 - November 1, 2018},
  pages     = {253--260},
  publisher = {Association for Computational Linguistics},
  year      = {2018},
  url       = {https://doi.org/10.18653/v1/w18-6326},
  doi       = {10.18653/v1/w18-6326},
  timestamp = {Thu, 17 Feb 2022 16:43:16 +0100},
  biburl    = {https://dblp.org/rec/conf/wmt/LibovickyHM18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
%48
@inproceedings{48_DBLP:conf/aaai/WangX21,
  author    = {Dexin Wang and
               Deyi Xiong},
  title     = {Efficient Object-Level Visual Context Modeling for Multimodal Machine
               Translation: Masking Irrelevant Objects Helps Grounding},
  booktitle = {Thirty-Fifth {AAAI} Conference on Artificial Intelligence, {AAAI}
               2021, Thirty-Third Conference on Innovative Applications of Artificial
               Intelligence, {IAAI} 2021, The Eleventh Symposium on Educational Advances
               in Artificial Intelligence, {EAAI} 2021, Virtual Event, February 2-9,
               2021},
  pages     = {2720--2728},
  publisher = {{AAAI} Press},
  year      = {2021},
  url       = {https://ojs.aaai.org/index.php/AAAI/article/view/16376},
  timestamp = {Wed, 02 Jun 2021 18:09:11 +0200},
  biburl    = {https://dblp.org/rec/conf/aaai/WangX21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
%49
@inproceedings{49_wang-zhang-2022-addressing,
    title = "Addressing Asymmetry in Multilingual Neural Machine Translation with Fuzzy Task Clustering",
    author = "Wang, Qian  and
      Zhang, Jiajun",
    booktitle = "Proceedings of the 29th International Conference on Computational Linguistics",
    month = oct,
    year = "2022",
    address = "Gyeongju, Republic of Korea",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2022.coling-1.455",
    pages = "5129--5141",
    abstract = "Multilingual neural machine translation (NMT) enables positive knowledge transfer among multiple translation tasks with a shared underlying model, but a unified multilingual model usually suffers from capacity bottleneck when tens or hundreds of languages are involved. A possible solution is to cluster languages and train individual model for each cluster. However, the existing clustering methods based on language similarity cannot handle the asymmetric problem in multilingual NMT, i.e., one translation task A can benefit from another translation task B but task B will be harmed by task A. To address this problem, we propose a fuzzy task clustering method for multilingual NMT. Specifically, we employ task affinity, defined as the loss change of one translation task caused by the training of another, as the clustering criterion. Next, we cluster the translation tasks based on the task affinity, such that tasks from the same cluster can benefit each other. For each cluster, we further find out a set of auxiliary translation tasks that benefit the tasks in this cluster. In this way, the model for each cluster is trained not only on the tasks in the cluster but also on the auxiliary tasks. We conduct extensive experiments for one-to-many, manyto-one, and many-to-many translation scenarios to verify the effectiveness of our method.",
}
%50
%亢晓勉, 宗成庆. 基于篇章结构多任务学习的神经机器翻译[J]. 软件学报, 2022, 33(10): 3806-3818. http://www.jos.org.cn/1000-9825/6316.htm    复制到剪切板
%Kang XM, Zong CQ. Neural Machine Translation Based on Multi-task Learning of Discourse Structure[J]. Journal of Software, 2022, 33(10): 3806-3818(in Chinese). http://www.jos.org.cn/1000-9825/6316.htm
@article{50_kang_zong_2022,
    title={基于篇章结构多任务学习的神经机器翻译},
    author={亢晓勉 and 宗成庆},
    key={Kang Xiao Mian Zong Cheng Qing},
    journal={软件学报},
    volume={33},
    number={10},
    pages={3806--3818},
    year={2022}
}
%51
@inproceedings{51_long-etal-2021-generative,
    title = "Generative Imagination Elevates Machine Translation",
    author = "Long, Quanyu  and
      Wang, Mingxuan  and
      Li, Lei",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.457",
    doi = "10.18653/v1/2021.naacl-main.457",
    pages = "5738--5748",
    abstract = "There are common semantics shared across text and images. Given a sentence in a source language, whether depicting the visual scene helps translation into a target language? Existing multimodal neural machine translation methods (MNMT) require triplets of bilingual sentence - image for training and tuples of source sentence - image for inference. In this paper, we propose ImagiT, a novel machine translation method via visual imagination. ImagiT first learns to generate visual representation from the source sentence, and then utilizes both source sentence and the {``}imagined representation{''} to produce a target translation. Unlike previous methods, it only needs the source sentence at the inference time. Experiments demonstrate that ImagiT benefits from visual imagination and significantly outperforms the text-only neural machine translation baselines. Further analysis reveals that the imagination process in ImagiT helps fill in missing information when performing the degradation strategy.",
}
%52
@article{52_DBLP:journals/corr/ElliottFH15,
  author    = {Desmond Elliott and
               Stella Frank and
               Eva Hasler},
  title     = {Multi-Language Image Description with Neural Sequence Models},
  journal   = {CoRR},
  volume    = {abs/1510.04709},
  year      = {2015},
  url       = {http://arxiv.org/abs/1510.04709},
  eprinttype = {arXiv},
  eprint    = {1510.04709},
  timestamp = {Mon, 13 Aug 2018 16:46:09 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/ElliottFH15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
%53
@inproceedings{53_caglayan-etal-2019-probing,
    title = "Probing the Need for Visual Context in Multimodal Machine Translation",
    author = {Caglayan, Ozan  and
      Madhyastha, Pranava  and
      Specia, Lucia  and
      Barrault, Lo{\"\i}c},
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1422",
    doi = "10.18653/v1/N19-1422",
    pages = "4159--4170",
    abstract = "Current work on multimodal machine translation (MMT) has suggested that the visual modality is either unnecessary or only marginally beneficial. We posit that this is a consequence of the very simple, short and repetitive sentences used in the only available dataset for the task (Multi30K), rendering the source text sufficient as context. In the general case, however, we believe that it is possible to combine visual and textual information in order to ground translations. In this paper we probe the contribution of the visual modality to state-of-the-art MMT models by conducting a systematic analysis where we partially deprive the models from source-side textual context. Our results show that under limited textual context, models are capable of leveraging the visual input to generate better translations. This contradicts the current belief that MMT models disregard the visual modality because of either the quality of the image features or the way they are integrated into the model.",
}
%54
@article{54_DBLP:journals/mt/NakayamaN17,
  author    = {Hideki Nakayama and
               Noriki Nishida},
  title     = {Zero-resource machine translation by multimodal encoder-decoder network
               with multimedia pivot},
  journal   = {Mach. Transl.},
  volume    = {31},
  number    = {1-2},
  pages     = {49--64},
  year      = {2017},
  url       = {https://doi.org/10.1007/s10590-017-9197-z},
  doi       = {10.1007/s10590-017-9197-z},
  timestamp = {Sat, 05 Sep 2020 17:50:05 +0200},
  biburl    = {https://dblp.org/rec/journals/mt/NakayamaN17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
%55
@inproceedings{55_DBLP:conf/ijcai/ChenJF19,
  author    = {Shizhe Chen and
               Qin Jin and
               Jianlong Fu},
  editor    = {Sarit Kraus},
  title     = {From Words to Sentences: {A} Progressive Learning Approach for Zero-resource
               Machine Translation with Visual Pivots},
  booktitle = {Proceedings of the Twenty-Eighth International Joint Conference on
               Artificial Intelligence, {IJCAI} 2019, Macao, China, August 10-16,
               2019},
  pages     = {4932--4938},
  publisher = {ijcai.org},
  year      = {2019},
  url       = {https://doi.org/10.24963/ijcai.2019/685},
  doi       = {10.24963/ijcai.2019/685},
  timestamp = {Tue, 20 Aug 2019 16:19:19 +0200},
  biburl    = {https://dblp.org/rec/conf/ijcai/ChenJF19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
%56
@inproceedings{56_zhou-etal-2018-visual,
    title = "A Visual Attention Grounding Neural Model for Multimodal Machine Translation",
    author = "Zhou, Mingyang  and
      Cheng, Runxiang  and
      Lee, Yong Jae  and
      Yu, Zhou",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1400",
    doi = "10.18653/v1/D18-1400",
    pages = "3643--3653",
    abstract = "We introduce a novel multimodal machine translation model that utilizes parallel visual and textual information. Our model jointly optimizes the learning of a shared visual-language embedding and a translator. The model leverages a visual attention grounding mechanism that links the visual semantics with the corresponding textual semantics. Our approach achieves competitive state-of-the-art results on the Multi30K and the Ambiguous COCO datasets. We also collected a new multilingual multimodal product description dataset to simulate a real-world international online shopping scenario. On this dataset, our visual attention grounding model outperforms other methods by a large margin.",
}
%57 visual_bert
@article{57_visual_bert,
  author    = {Liunian Harold Li and
               Mark Yatskar and
               Da Yin and
               Cho{-}Jui Hsieh and
               Kai{-}Wei Chang},
  title     = {VisualBERT: {A} Simple and Performant Baseline for Vision and Language},
  journal   = {CoRR},
  volume    = {abs/1908.03557},
  year      = {2019},
  url       = {http://arxiv.org/abs/1908.03557},
  eprinttype = {arXiv},
  eprint    = {1908.03557},
  timestamp = {Mon, 19 Aug 2019 13:21:03 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1908-03557.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
%58 unicoder-vl
@inproceedings{58_unicoder_vl,
  author    = {Gen Li and
               Nan Duan and
               Yuejian Fang and
               Ming Gong and
               Daxin Jiang},
  title     = {Unicoder-VL: {A} Universal Encoder for Vision and Language by Cross-Modal
               Pre-Training},
  booktitle = {The Thirty-Fourth {AAAI} Conference on Artificial Intelligence, {AAAI}
               2020, The Thirty-Second Innovative Applications of Artificial Intelligence
               Conference, {IAAI} 2020, The Tenth {AAAI} Symposium on Educational
               Advances in Artificial Intelligence, {EAAI} 2020, New York, NY, USA,
               February 7-12, 2020},
  pages     = {11336--11344},
  publisher = {{AAAI} Press},
  year      = {2020},
  url       = {https://ojs.aaai.org/index.php/AAAI/article/view/6795},
  timestamp = {Mon, 07 Mar 2022 16:57:57 +0100},
  biburl    = {https://dblp.org/rec/conf/aaai/LiDFGJ20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
%59 vl-bert
@inproceedings{59_vl_bert,
  author    = {Weijie Su and
               Xizhou Zhu and
               Yue Cao and
               Bin Li and
               Lewei Lu and
               Furu Wei and
               Jifeng Dai},
  title     = {{VL-BERT:} Pre-training of Generic Visual-Linguistic Representations},
  booktitle = {8th International Conference on Learning Representations, {ICLR} 2020,
               Addis Ababa, Ethiopia, April 26-30, 2020},
  publisher = {OpenReview.net},
  year      = {2020},
  url       = {https://openreview.net/forum?id=SygXPaEYvH},
  timestamp = {Tue, 12 Apr 2022 21:46:12 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/SuZCLLWD20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
%60 uniter
@inproceedings{60_uniter,
  author    = {Yen{-}Chun Chen and
               Linjie Li and
               Licheng Yu and
               Ahmed El Kholy and
               Faisal Ahmed and
               Zhe Gan and
               Yu Cheng and
               Jingjing Liu},
  editor    = {Andrea Vedaldi and
               Horst Bischof and
               Thomas Brox and
               Jan{-}Michael Frahm},
  title     = {{UNITER:} UNiversal Image-TExt Representation Learning},
  booktitle = {Computer Vision - {ECCV} 2020 - 16th European Conference, Glasgow,
               UK, August 23-28, 2020, Proceedings, Part {XXX}},
  series    = {Lecture Notes in Computer Science},
  volume    = {12375},
  pages     = {104--120},
  publisher = {Springer},
  year      = {2020},
  url       = {https://doi.org/10.1007/978-3-030-58577-8\_7},
  doi       = {10.1007/978-3-030-58577-8\_7},
  timestamp = {Sun, 02 Oct 2022 15:59:30 +0200},
  biburl    = {https://dblp.org/rec/conf/eccv/ChenLYK0G0020.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
%61 vilbert
@inproceedings{61_vilbert,
  author    = {Jiasen Lu and
               Dhruv Batra and
               Devi Parikh and
               Stefan Lee},
  editor    = {Hanna M. Wallach and
               Hugo Larochelle and
               Alina Beygelzimer and
               Florence d'Alch{\'{e}}{-}Buc and
               Emily B. Fox and
               Roman Garnett},
  title     = {ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations
               for Vision-and-Language Tasks},
  booktitle = {Advances in Neural Information Processing Systems 32: Annual Conference
               on Neural Information Processing Systems 2019, NeurIPS 2019, December
               8-14, 2019, Vancouver, BC, Canada},
  pages     = {13--23},
  year      = {2019},
  url       = {https://proceedings.neurips.cc/paper/2019/hash/c74d97b01eae257e44aa9d5bade97baf-Abstract.html},
  timestamp = {Mon, 16 May 2022 15:41:51 +0200},
  biburl    = {https://dblp.org/rec/conf/nips/LuBPL19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
%62 lxmert
@inproceedings{62_lxmert,
    title = "{LXMERT}: Learning Cross-Modality Encoder Representations from Transformers",
    author = "Tan, Hao  and
      Bansal, Mohit",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1514",
    doi = "10.18653/v1/D19-1514",
    pages = "5100--5111",
    abstract = "Vision-and-language reasoning requires an understanding of visual concepts, language semantics, and, most importantly, the alignment and relationships between these two modalities. We thus propose the LXMERT (Learning Cross-Modality Encoder Representations from Transformers) framework to learn these vision-and-language connections. In LXMERT, we build a large-scale Transformer model that consists of three encoders: an object relationship encoder, a language encoder, and a cross-modality encoder. Next, to endow our model with the capability of connecting vision and language semantics, we pre-train the model with large amounts of image-and-sentence pairs, via five diverse representative pre-training tasks: masked language modeling, masked object prediction (feature regression and label classification), cross-modality matching, and image question answering. These tasks help in learning both intra-modality and cross-modality relationships. After fine-tuning from our pre-trained parameters, our model achieves the state-of-the-art results on two visual question answering datasets (i.e., VQA and GQA). We also show the generalizability of our pre-trained cross-modality model by adapting it to a challenging visual-reasoning task, NLVR2, and improve the previous best result by 22{\%} absolute (54{\%} to 76{\%}). Lastly, we demonstrate detailed ablation studies to prove that both our novel model components and pre-training strategies significantly contribute to our strong results. Code and pre-trained models publicly available at: https://github.com/airsplay/lxmert",
}
%63 albef
@inproceedings{63_albef,
  author    = {Junnan Li and
               Ramprasaath R. Selvaraju and
               Akhilesh Gotmare and
               Shafiq R. Joty and
               Caiming Xiong and
               Steven Chu{-}Hong Hoi},
  editor    = {Marc'Aurelio Ranzato and
               Alina Beygelzimer and
               Yann N. Dauphin and
               Percy Liang and
               Jennifer Wortman Vaughan},
  title     = {Align before Fuse: Vision and Language Representation Learning with
               Momentum Distillation},
  booktitle = {Advances in Neural Information Processing Systems 34: Annual Conference
               on Neural Information Processing Systems 2021, NeurIPS 2021, December
               6-14, 2021, virtual},
  pages     = {9694--9705},
  year      = {2021},
  url       = {https://proceedings.neurips.cc/paper/2021/hash/505259756244493872b7709a8a01b536-Abstract.html},
  timestamp = {Tue, 03 May 2022 16:20:47 +0200},
  biburl    = {https://dblp.org/rec/conf/nips/LiSGJXH21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
%64 pixel-bert
@article{64_pixel_bert,
  author    = {Zhicheng Huang and
               Zhaoyang Zeng and
               Bei Liu and
               Dongmei Fu and
               Jianlong Fu},
  title     = {Pixel-BERT: Aligning Image Pixels with Text by Deep Multi-Modal Transformers},
  journal   = {CoRR},
  volume    = {abs/2004.00849},
  year      = {2020},
  url       = {https://arxiv.org/abs/2004.00849},
  eprinttype = {arXiv},
  eprint    = {2004.00849},
  timestamp = {Fri, 13 Aug 2021 14:56:28 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2004-00849.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
%65 vilt
@inproceedings{65_vilt,
  author    = {Wonjae Kim and
               Bokyung Son and
               Ildoo Kim},
  editor    = {Marina Meila and
               Tong Zhang},
  title     = {ViLT: Vision-and-Language Transformer Without Convolution or Region
               Supervision},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning,
               {ICML} 2021, 18-24 July 2021, Virtual Event},
  series    = {Proceedings of Machine Learning Research},
  volume    = {139},
  pages     = {5583--5594},
  publisher = {{PMLR}},
  year      = {2021},
  url       = {http://proceedings.mlr.press/v139/kim21k.html},
  timestamp = {Wed, 25 Aug 2021 17:11:17 +0200},
  biburl    = {https://dblp.org/rec/conf/icml/KimSK21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
%66 show and tell
@inproceedings{66_DBLP:conf/cvpr/VinyalsTBE15,
  author    = {Oriol Vinyals and
               Alexander Toshev and
               Samy Bengio and
               Dumitru Erhan},
  title     = {Show and tell: {A} neural image caption generator},
  booktitle = {{IEEE} Conference on Computer Vision and Pattern Recognition, {CVPR}
               2015, Boston, MA, USA, June 7-12, 2015},
  pages     = {3156--3164},
  publisher = {{IEEE} Computer Society},
  year      = {2015},
  url       = {https://doi.org/10.1109/CVPR.2015.7298935},
  doi       = {10.1109/CVPR.2015.7298935},
  timestamp = {Wed, 16 Oct 2019 14:14:50 +0200},
  biburl    = {https://dblp.org/rec/conf/cvpr/VinyalsTBE15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
%67 show attend and tell
@inproceedings{67_DBLP:conf/icml/XuBKCCSZB15,
  author    = {Kelvin Xu and
               Jimmy Ba and
               Ryan Kiros and
               Kyunghyun Cho and
               Aaron C. Courville and
               Ruslan Salakhutdinov and
               Richard S. Zemel and
               Yoshua Bengio},
  editor    = {Francis R. Bach and
               David M. Blei},
  title     = {Show, Attend and Tell: Neural Image Caption Generation with Visual
               Attention},
  booktitle = {Proceedings of the 32nd International Conference on Machine Learning,
               {ICML} 2015, Lille, France, 6-11 July 2015},
  series    = {{JMLR} Workshop and Conference Proceedings},
  volume    = {37},
  pages     = {2048--2057},
  publisher = {JMLR.org},
  year      = {2015},
  url       = {http://proceedings.mlr.press/v37/xuc15.html},
  timestamp = {Wed, 29 May 2019 08:41:46 +0200},
  biburl    = {https://dblp.org/rec/conf/icml/XuBKCCSZB15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
%68
@article{68_DBLP:journals/corr/KirosSZ14,
  author    = {Ryan Kiros and
               Ruslan Salakhutdinov and
               Richard S. Zemel},
  title     = {Unifying Visual-Semantic Embeddings with Multimodal Neural Language
               Models},
  journal   = {CoRR},
  volume    = {abs/1411.2539},
  year      = {2014},
  url       = {http://arxiv.org/abs/1411.2539},
  eprinttype = {arXiv},
  eprint    = {1411.2539},
  timestamp = {Mon, 13 Aug 2018 16:47:15 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/KirosSZ14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
%69 
@inproceedings{69_DBLP:conf/cvpr/LuXPS17,
  author    = {Jiasen Lu and
               Caiming Xiong and
               Devi Parikh and
               Richard Socher},
  title     = {Knowing When to Look: Adaptive Attention via a Visual Sentinel for
               Image Captioning},
  booktitle = {2017 {IEEE} Conference on Computer Vision and Pattern Recognition,
               {CVPR} 2017, Honolulu, HI, USA, July 21-26, 2017},
  pages     = {3242--3250},
  publisher = {{IEEE} Computer Society},
  year      = {2017},
  url       = {https://doi.org/10.1109/CVPR.2017.345},
  doi       = {10.1109/CVPR.2017.345},
  timestamp = {Wed, 16 Oct 2019 14:14:50 +0200},
  biburl    = {https://dblp.org/rec/conf/cvpr/LuXPS17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
% 70 many-to-many
@inproceedings{70_pan-etal-2021-contrastive,
    title = "Contrastive Learning for Many-to-many Multilingual Neural Machine Translation",
    author = "Pan, Xiao  and
      Wang, Mingxuan  and
      Wu, Liwei  and
      Li, Lei",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.21",
    doi = "10.18653/v1/2021.acl-long.21",
    pages = "244--258",
    abstract = "Existing multilingual machine translation approaches mainly focus on English-centric directions, while the non-English directions still lag behind. In this work, we aim to build a many-to-many translation system with an emphasis on the quality of non-English language directions. Our intuition is based on the hypothesis that a universal cross-language representation leads to better multilingual translation performance. To this end, we propose mRASP2, a training method to obtain a single unified multilingual translation model. mRASP2 is empowered by two techniques: a) a contrastive learning scheme to close the gap among representations of different languages, and b) data augmentation on both multiple parallel and monolingual data to further align token representations. For English-centric directions, mRASP2 achieves competitive or even better performance than a strong pre-trained model mBART on tens of WMT benchmarks. For non-English directions, mRASP2 achieves an improvement of average 10+ BLEU compared with the multilingual baseline",
}
% 80 unimo
@inproceedings{80_li-etal-2021-unimo,
    title = "{UNIMO}: Towards Unified-Modal Understanding and Generation via Cross-Modal Contrastive Learning",
    author = "Li, Wei  and
      Gao, Can  and
      Niu, Guocheng  and
      Xiao, Xinyan  and
      Liu, Hao  and
      Liu, Jiachen  and
      Wu, Hua  and
      Wang, Haifeng",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.202",
    doi = "10.18653/v1/2021.acl-long.202",
    pages = "2592--2607",
    abstract = "Existed pre-training methods either focus on single-modal tasks or multi-modal tasks, and cannot effectively adapt to each other. They can only utilize single-modal data (i.e., text or image) or limited multi-modal data (i.e., image-text pairs). In this work, we propose a UNIfied-MOdal pre-training architecture, namely UNIMO, which can effectively adapt to both single-modal and multi-modal understanding and generation tasks. Large scale of free text corpus and image collections are utilized to improve the capability of visual and textual understanding, and cross-modal contrastive learning (CMCL) is leveraged to align the textual and visual information into a unified semantic space, over a corpus of image-text pairs augmented with related images and texts. With the help of rich non-paired single-modal data, our model is able to learn more generalizable representations, by allowing textual knowledge and visual knowledge to enhance each other in the unified semantic space. The experimental results show that UNIMO greatly improves the performance of several single-modal and multi-modal downstream tasks. Our code and pre-trained models are public at \url{https://github.com/PaddlePaddle/Research/tree/master/NLP/UNIMO}.",
}
%81 the survey
@article{81_DBLP:journals/mt/SulubacakCGREST20,
  author    = {Umut Sulubacak and
               Ozan Caglayan and
               Stig{-}Arne Gr{\"{o}}nroos and
               Aku Rouhe and
               Desmond Elliott and
               Lucia Specia and
               J{\"{o}}rg Tiedemann},
  title     = {Multimodal machine translation through visuals and speech},
  journal   = {Mach. Transl.},
  volume    = {34},
  number    = {2-3},
  pages     = {97--147},
  year      = {2020},
  url       = {https://doi.org/10.1007/s10590-020-09250-0},
  doi       = {10.1007/s10590-020-09250-0},
  timestamp = {Sun, 02 Oct 2022 15:44:08 +0200},
  biburl    = {https://dblp.org/rec/journals/mt/SulubacakCGREST20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
%82
@inproceedings{82_DBLP:conf/icassp/Vidal97,
  author    = {Enrique Vidal},
  title     = {Finite-state speech-to-speech translation},
  booktitle = {1997 {IEEE} International Conference on Acoustics, Speech, and Signal
               Processing, {ICASSP} '97, Munich, Germany, April 21-24, 1997},
  pages     = {111--114},
  publisher = {{IEEE} Computer Society},
  year      = {1997},
  url       = {https://doi.org/10.1109/ICASSP.1997.599563},
  doi       = {10.1109/ICASSP.1997.599563},
  timestamp = {Wed, 16 Oct 2019 14:14:52 +0200},
  biburl    = {https://dblp.org/rec/conf/icassp/Vidal97.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
%83
@inproceedings{83_DBLP:conf/icassp/Ney99,
  author    = {Hermann Ney},
  title     = {Speech translation: coupling of recognition and translation},
  booktitle = {Proceedings of the 1999 {IEEE} International Conference on Acoustics,
               Speech, and Signal Processing, {ICASSP} '99, Phoenix, Arizona, USA,
               March 15-19, 1999},
  pages     = {517--520},
  publisher = {{IEEE} Computer Society},
  year      = {1999},
  url       = {https://doi.org/10.1109/ICASSP.1999.758176},
  doi       = {10.1109/ICASSP.1999.758176},
  timestamp = {Wed, 16 Oct 2019 14:14:52 +0200},
  biburl    = {https://dblp.org/rec/conf/icassp/Ney99.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
%84
@inproceedings{84_DBLP:conf/interspeech/WeissCJWC17,
  author    = {Ron J. Weiss and
               Jan Chorowski and
               Navdeep Jaitly and
               Yonghui Wu and
               Zhifeng Chen},
  editor    = {Francisco Lacerda},
  title     = {Sequence-to-Sequence Models Can Directly Translate Foreign Speech},
  booktitle = {Interspeech 2017, 18th Annual Conference of the International Speech
               Communication Association, Stockholm, Sweden, August 20-24, 2017},
  pages     = {2625--2629},
  publisher = {{ISCA}},
  year      = {2017},
  url       = {http://www.isca-speech.org/archive/Interspeech\_2017/abstracts/0503.html},
  timestamp = {Sun, 02 Oct 2022 16:08:37 +0200},
  biburl    = {https://dblp.org/rec/conf/interspeech/WeissCJWC17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
%85
@inproceedings{85_DBLP:conf/icassp/BerardBKP18,
  author    = {Alexandre Berard and
               Laurent Besacier and
               Ali Can Kocabiyikoglu and
               Olivier Pietquin},
  title     = {End-to-End Automatic Speech Translation of Audiobooks},
  booktitle = {2018 {IEEE} International Conference on Acoustics, Speech and Signal
               Processing, {ICASSP} 2018, Calgary, AB, Canada, April 15-20, 2018},
  pages     = {6224--6228},
  publisher = {{IEEE}},
  year      = {2018},
  url       = {https://doi.org/10.1109/ICASSP.2018.8461690},
  doi       = {10.1109/ICASSP.2018.8461690},
  timestamp = {Wed, 16 Oct 2019 14:14:52 +0200},
  biburl    = {https://dblp.org/rec/conf/icassp/BerardBKP18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
%86
@inproceedings{86_DBLP:conf/lrec/LisonT16,
  author    = {Pierre Lison and
               J{\"{o}}rg Tiedemann},
  editor    = {Nicoletta Calzolari and
               Khalid Choukri and
               Thierry Declerck and
               Sara Goggi and
               Marko Grobelnik and
               Bente Maegaard and
               Joseph Mariani and
               H{\'{e}}l{\`{e}}ne Mazo and
               Asunci{\'{o}}n Moreno and
               Jan Odijk and
               Stelios Piperidis},
  title     = {OpenSubtitles2016: Extracting Large Parallel Corpora from Movie and
               {TV} Subtitles},
  booktitle = {Proceedings of the Tenth International Conference on Language Resources
               and Evaluation {LREC} 2016, Portoro{\v{z}}, Slovenia, May 23-28, 2016},
  publisher = {European Language Resources Association {(ELRA)}},
  year      = {2016},
  url       = {http://www.lrec-conf.org/proceedings/lrec2016/summaries/947.html},
  timestamp = {Mon, 19 Aug 2019 15:22:18 +0200},
  biburl    = {https://dblp.org/rec/conf/lrec/LisonT16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
%87
@article{87_DBLP:journals/corr/abs-1811-00347,
  author    = {Ramon Sanabria and
               Ozan Caglayan and
               Shruti Palaskar and
               Desmond Elliott and
               Lo{\"{\i}}c Barrault and
               Lucia Specia and
               Florian Metze},
  title     = {How2: {A} Large-scale Dataset for Multimodal Language Understanding},
  journal   = {CoRR},
  volume    = {abs/1811.00347},
  year      = {2018},
  url       = {http://arxiv.org/abs/1811.00347},
  eprinttype = {arXiv},
  eprint    = {1811.00347},
  timestamp = {Thu, 22 Nov 2018 17:58:30 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1811-00347.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
%88
@inproceedings{88_DBLP:conf/iccv/WangWCLWW19,
  author    = {Xin Wang and
               Jiawei Wu and
               Junkun Chen and
               Lei Li and
               Yuan{-}Fang Wang and
               William Yang Wang},
  title     = {VaTeX: {A} Large-Scale, High-Quality Multilingual Dataset for Video-and-Language
               Research},
  booktitle = {2019 {IEEE/CVF} International Conference on Computer Vision, {ICCV}
               2019, Seoul, Korea (South), October 27 - November 2, 2019},
  pages     = {4580--4590},
  publisher = {{IEEE}},
  year      = {2019},
  url       = {https://doi.org/10.1109/ICCV.2019.00468},
  doi       = {10.1109/ICCV.2019.00468},
  timestamp = {Fri, 25 Nov 2022 13:54:42 +0100},
  biburl    = {https://dblp.org/rec/conf/iccv/WangWCLWW19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
%89
@inproceedings{89_DBLP:conf/eacl/CaglayanKAMEES21,
  author    = {Ozan Caglayan and
               Menekse Kuyu and
               Mustafa Sercan Amac and
               Pranava Madhyastha and
               Erkut Erdem and
               Aykut Erdem and
               Lucia Specia},
  editor    = {Paola Merlo and
               J{\"{o}}rg Tiedemann and
               Reut Tsarfaty},
  title     = {Cross-lingual Visual Pre-training for Multimodal Machine Translation},
  booktitle = {Proceedings of the 16th Conference of the European Chapter of the
               Association for Computational Linguistics: Main Volume, {EACL} 2021,
               Online, April 19 - 23, 2021},
  pages     = {1317--1324},
  publisher = {Association for Computational Linguistics},
  year      = {2021},
  url       = {https://doi.org/10.18653/v1/2021.eacl-main.112},
  doi       = {10.18653/v1/2021.eacl-main.112},
  timestamp = {Thu, 20 Jan 2022 10:02:52 +0100},
  biburl    = {https://dblp.org/rec/conf/eacl/CaglayanKAMEES21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
%90
@inproceedings{90_wang-etal-2021-make,
    title = "Make the Blind Translator See The World: A Novel Transfer Learning Solution for Multimodal Machine Translation",
    author = "Wang, Minghan  and
      Guo, Jiaxin  and
      Chen, Yimeng  and
      Su, Chang  and
      Zhang, Min  and
      Tao, Shimin  and
      Yang, Hao",
    booktitle = "Proceedings of Machine Translation Summit XVIII: Research Track",
    month = aug,
    year = "2021",
    address = "Virtual",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2021.mtsummit-research.12",
    pages = "139--149",
    abstract = "Based on large-scale pretrained networks and the liability to be easily overfitting with limited labelled training data of multimodal translation (MMT) is a critical issue in MMT. To this end and we propose a transfer learning solution. Specifically and 1) A vanilla Transformer is pre-trained on massive bilingual text-only corpus to obtain prior knowledge; 2) A multimodal Transformer named VLTransformer is proposed with several components incorporated visual contexts; and 3) The parameters of VLTransformer are initialized with the pre-trained vanilla Transformer and then being fine-tuned on MMT tasks with a newly proposed method named cross-modal masking which forces the model to learn from both modalities. We evaluated on the Multi30k en-de and en-fr dataset and improving up to 8{\%} BLEU score compared with the SOTA performance. The experimental result demonstrates that performing transfer learning with monomodal pre-trained NMT model on multimodal NMT tasks can obtain considerable boosts.",
}
%91
@inproceedings{91_susanto-etal-2021-rakutens,
    title = "Rakuten{'}s Participation in {WAT} 2021: Examining the Effectiveness of Pre-trained Models for Multilingual and Multimodal Machine Translation",
    author = "Susanto, Raymond Hendy  and
      Wang, Dongzhe  and
      Yadav, Sunil  and
      Jain, Mausam  and
      Htun, Ohnmar",
    booktitle = "Proceedings of the 8th Workshop on Asian Translation (WAT2021)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.wat-1.9",
    doi = "10.18653/v1/2021.wat-1.9",
    pages = "96--105",
    abstract = "This paper introduces our neural machine translation systems{'} participation in the WAT 2021 shared translation tasks (team ID: sakura). We participated in the (i) NICT-SAP, (ii) Japanese-English multimodal translation, (iii) Multilingual Indic, and (iv) Myanmar-English translation tasks. Multilingual approaches such as mBART (Liu et al., 2020) are capable of pre-training a complete, multilingual sequence-to-sequence model through denoising objectives, making it a great starting point for building multilingual translation systems. Our main focus in this work is to investigate the effectiveness of multilingual finetuning on such a multilingual language model on various translation tasks, including low-resource, multimodal, and mixed-domain translation. We further explore a multimodal approach based on universal visual representation (Zhang et al., 2019) and compare its performance against a unimodal approach based on mBART alone.",
}
%92
@inproceedings{92_yawei-fan-2021-probing,
    title = "Probing Multi-modal Machine Translation with Pre-trained Language Model",
    author = "Yawei, Kong  and
      Fan, Kai",
    booktitle = "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-acl.323",
    doi = "10.18653/v1/2021.findings-acl.323",
    pages = "3689--3699",
}
%93
@article{93_DBLP:journals/access/HirasawaKIK22,
  author    = {Tosho Hirasawa and
               Masahiro Kaneko and
               Aizhan Imankulova and
               Mamoru Komachi},
  title     = {Pre-Trained Word Embedding and Language Model Improve Multimodal Machine
               Translation: {A} Case Study in Multi30K},
  journal   = {{IEEE} Access},
  volume    = {10},
  pages     = {67653--67668},
  year      = {2022},
  url       = {https://doi.org/10.1109/ACCESS.2022.3185243},
  doi       = {10.1109/ACCESS.2022.3185243},
  timestamp = {Mon, 25 Jul 2022 08:39:29 +0200},
  biburl    = {https://dblp.org/rec/journals/access/HirasawaKIK22.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
%94
@inproceedings{94_DBLP:conf/aaai/YangCZ020,
  author    = {Pengcheng Yang and
               Boxing Chen and
               Pei Zhang and
               Xu Sun},
  title     = {Visual Agreement Regularized Training for Multi-Modal Machine Translation},
  booktitle = {The Thirty-Fourth {AAAI} Conference on Artificial Intelligence, {AAAI}
               2020, The Thirty-Second Innovative Applications of Artificial Intelligence
               Conference, {IAAI} 2020, The Tenth {AAAI} Symposium on Educational
               Advances in Artificial Intelligence, {EAAI} 2020, New York, NY, USA,
               February 7-12, 2020},
  pages     = {9418--9425},
  publisher = {{AAAI} Press},
  year      = {2020},
  url       = {https://ojs.aaai.org/index.php/AAAI/article/view/6484},
  timestamp = {Mon, 07 Mar 2022 16:58:05 +0100},
  biburl    = {https://dblp.org/rec/conf/aaai/YangCZ020.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
%95
@inproceedings{95_zheng-etal-2018-ensemble,
    title = "Ensemble Sequence Level Training for Multimodal {MT}: {OSU}-{B}aidu {WMT}18 Multimodal Machine Translation System Report",
    author = "Zheng, Renjie  and
      Yang, Yilin  and
      Ma, Mingbo  and
      Huang, Liang",
    booktitle = "Proceedings of the Third Conference on Machine Translation: Shared Task Papers",
    month = oct,
    year = "2018",
    address = "Belgium, Brussels",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-6443",
    doi = "10.18653/v1/W18-6443",
    pages = "632--636",
    abstract = "This paper describes multimodal machine translation systems developed jointly by Oregon State University and Baidu Research for WMT 2018 Shared Task on multimodal translation. In this paper, we introduce a simple approach to incorporate image information by feeding image features to the decoder side. We also explore different sequence level training methods including scheduled sampling and reinforcement learning which lead to substantial improvements. Our systems ensemble several models using different architectures and training methods and achieve the best performance for three subtasks: En-De and En-Cs in task 1 and (En+De+Fr)-Cs task 1B.",
}
%96
@inproceedings{96_zhao-etal-2020-double,
    title = "Double Attention-based Multimodal Neural Machine Translation with Semantic Image Regions",
    author = "Zhao, Yuting  and
      Komachi, Mamoru  and
      Kajiwara, Tomoyuki  and
      Chu, Chenhui",
    booktitle = "Proceedings of the 22nd Annual Conference of the European Association for Machine Translation",
    month = nov,
    year = "2020",
    address = "Lisboa, Portugal",
    publisher = "European Association for Machine Translation",
    url = "https://aclanthology.org/2020.eamt-1.12",
    pages = "105--114",
    abstract = "Existing studies on multimodal neural machine translation (MNMT) have mainly focused on the effect of combining visual and textual modalities to improve translations. However, it has been suggested that the visual modality is only marginally beneficial. Conventional visual attention mechanisms have been used to select the visual features from equally-sized grids generated by convolutional neural networks (CNNs), and may have had modest effects on aligning the visual concepts associated with textual objects, because the grid visual features do not capture semantic information. In contrast, we propose the application of semantic image regions for MNMT by integrating visual and textual features using two individual attention mechanisms (double attention). We conducted experiments on the Multi30k dataset and achieved an improvement of 0.5 and 0.9 BLEU points for English-German and English-French translation tasks, compared with the MNMT with grid visual features. We also demonstrated concrete improvements on translation performance benefited from semantic image regions.",
}
%97 doubly attention rnn
@inproceedings{97_gain-etal-2021-iitp,
    title = "{IITP} at {WAT} 2021: System description for {E}nglish-{H}indi Multimodal Translation Task",
    author = "Gain, Baban  and
      Bandyopadhyay, Dibyanayan  and
      Ekbal, Asif",
    booktitle = "Proceedings of the 8th Workshop on Asian Translation (WAT2021)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.wat-1.18",
    doi = "10.18653/v1/2021.wat-1.18",
    pages = "161--165",
    abstract = "Neural Machine Translation (NMT) is a predominant machine translation technology nowadays because of its end-to-end trainable flexibility. However, NMT still struggles to translate properly in low-resource settings specifically on distant language pairs. One way to overcome this is to use the information from other modalities if available. The idea is that despite differences in languages, both the source and target language speakers see the same thing and the visual representation of both the source and target is the same, which can positively assist the system. Multimodal information can help the NMT system to improve the translation by removing ambiguity on some phrases or words. We participate in the 8th Workshop on Asian Translation (WAT - 2021) for English-Hindi multimodal translation task and achieve 42.47 and 37.50 BLEU points for Evaluation and Challenge subset, respectively.",
}
%98 doubly attention rnn
@inproceedings{98_laskar-etal-2021-improved,
    title = "Improved {E}nglish to {H}indi Multimodal Neural Machine Translation",
    author = "Laskar, Sahinur Rahman  and
      Khilji, Abdullah Faiz Ur Rahman  and
      Kaushik, Darsh  and
      Pakray, Partha  and
      Bandyopadhyay, Sivaji",
    booktitle = "Proceedings of the 8th Workshop on Asian Translation (WAT2021)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.wat-1.17",
    doi = "10.18653/v1/2021.wat-1.17",
    pages = "155--160",
    abstract = "Machine translation performs automatic translation from one natural language to another. Neural machine translation attains a state-of-the-art approach in machine translation, but it requires adequate training data, which is a severe problem for low-resource language pairs translation. The concept of multimodal is introduced in neural machine translation (NMT) by merging textual features with visual features to improve low-resource pair translation. WAT2021 (Workshop on Asian Translation 2021) organizes a shared task of multimodal translation for English to Hindi. We have participated the same with team name CNLP-NITS-PP in two submissions: multimodal and text-only NMT. This work investigates phrase pairs injection via data augmentation approach and attains improvement over our previous work at WAT2020 on the same task in both text-only and multimodal NMT. We have achieved second rank on the challenge test set for English to Hindi multimodal translation where Bilingual Evaluation Understudy (BLEU) score of 39.28, Rank-based Intuitive Bilingual Evaluation Score (RIBES) 0.792097, and Adequacy-Fluency Metrics (AMFM) score 0.830230 respectively.",
}
%99 doubly attention rnn
@inproceedings{99_DBLP:conf/wmt/CaglayanBBBWMHW18,
  author    = {Ozan Caglayan and
               Adrien Bardet and
               Fethi Bougares and
               Lo{\"{\i}}c Barrault and
               Kai Wang and
               Marc Masana and
               Luis Herranz and
               Joost van de Weijer},
  editor    = {Ondrej Bojar and
               Rajen Chatterjee and
               Christian Federmann and
               Mark Fishel and
               Yvette Graham and
               Barry Haddow and
               Matthias Huck and
               Antonio Jimeno{-}Yepes and
               Philipp Koehn and
               Christof Monz and
               Matteo Negri and
               Aur{\'{e}}lie N{\'{e}}v{\'{e}}ol and
               Mariana L. Neves and
               Matt Post and
               Lucia Specia and
               Marco Turchi and
               Karin Verspoor},
  title     = {{LIUM-CVC} Submissions for {WMT18} Multimodal Translation Task},
  booktitle = {Proceedings of the Third Conference on Machine Translation: Shared
               Task Papers, {WMT} 2018, Belgium, Brussels, October 31 - November
               1, 2018},
  pages     = {597--602},
  publisher = {Association for Computational Linguistics},
  year      = {2018},
  url       = {https://doi.org/10.18653/v1/w18-6438},
  doi       = {10.18653/v1/w18-6438},
  timestamp = {Thu, 17 Feb 2022 16:43:16 +0100},
  biburl    = {https://dblp.org/rec/conf/wmt/CaglayanBBBWMHW18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
%100 doubly attention rnn
@article{100_DBLP:journals/corr/CaglayanBB16,
  author    = {Ozan Caglayan and
               Lo{\"{\i}}c Barrault and
               Fethi Bougares},
  title     = {Multimodal Attention for Neural Machine Translation},
  journal   = {CoRR},
  volume    = {abs/1609.03976},
  year      = {2016},
  url       = {http://arxiv.org/abs/1609.03976},
  eprinttype = {arXiv},
  eprint    = {1609.03976},
  timestamp = {Mon, 13 Aug 2018 16:48:12 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/CaglayanBB16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
%101
@inproceedings{101_zhao-etal-2021-tmeku,
    title = "{TMEKU} System for the {WAT}2021 Multimodal Translation Task",
    author = "Zhao, Yuting  and
      Komachi, Mamoru  and
      Kajiwara, Tomoyuki  and
      Chu, Chenhui",
    booktitle = "Proceedings of the 8th Workshop on Asian Translation (WAT2021)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.wat-1.20",
    doi = "10.18653/v1/2021.wat-1.20",
    pages = "174--180",
    abstract = "We introduce our TMEKU system submitted to the English-Japanese Multimodal Translation Task for WAT 2021. We participated in the Flickr30kEnt-JP task and Ambiguous MSCOCO Multimodal task under the constrained condition using only the officially provided datasets. Our proposed system employs soft alignment of word-region for multimodal neural machine translation (MNMT). The experimental results evaluated on the BLEU metric provided by the WAT 2021 evaluation site show that the TMEKU system has achieved the best performance among all the participated systems. Further analysis of the case study demonstrates that leveraging word-region alignment between the textual and visual modalities is the key to performance enhancement in our TMEKU system, which leads to better visual information use.",
}
%102 doubly attention transformer
@inproceedings{102_gronroos-etal-2018-memad,
    title = "The {M}e{MAD} Submission to the {WMT}18 Multimodal Translation Task",
    author = {Gr{\"o}nroos, Stig-Arne  and
      Huet, Benoit  and
      Kurimo, Mikko  and
      Laaksonen, Jorma  and
      Merialdo, Bernard  and
      Pham, Phu  and
      Sj{\"o}berg, Mats  and
      Sulubacak, Umut  and
      Tiedemann, J{\"o}rg  and
      Troncy, Raphael  and
      V{\'a}zquez, Ra{\'u}l},
    booktitle = "Proceedings of the Third Conference on Machine Translation: Shared Task Papers",
    month = oct,
    year = "2018",
    address = "Belgium, Brussels",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-6439",
    doi = "10.18653/v1/W18-6439",
    pages = "603--611",
    abstract = "This paper describes the MeMAD project entry to the WMT Multimodal Machine Translation Shared Task. We propose adapting the Transformer neural machine translation (NMT) architecture to a multi-modal setting. In this paper, we also describe the preliminary experiments with text-only translation systems leading us up to this choice. We have the top scoring system for both English-to-German and English-to-French, according to the automatic metrics for flickr18. Our experiments show that the effect of the visual features in our system is small. Our largest gains come from the quality of the underlying text-only NMT system. We find that appropriate use of additional data is effective.",
}
%103 doubly attention transformer
@article{103_DBLP:journals/corr/abs-1807-11605,
  author    = {Hasan Sait Arslan and
               Mark Fishel and
               Gholamreza Anbarjafari},
  title     = {Doubly Attentive Transformer Machine Translation},
  journal   = {CoRR},
  volume    = {abs/1807.11605},
  year      = {2018},
  url       = {http://arxiv.org/abs/1807.11605},
  eprinttype = {arXiv},
  eprint    = {1807.11605},
  timestamp = {Mon, 13 Aug 2018 16:49:08 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1807-11605.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
%104
@article{104_DBLP:journals/corr/abs-2103-08862,
  author    = {Pengbo Liu and
               Hailong Cao and
               Tiejun Zhao},
  title     = {Gumbel-Attention for Multi-modal Machine Translation},
  journal   = {CoRR},
  volume    = {abs/2103.08862},
  year      = {2021},
  url       = {https://arxiv.org/abs/2103.08862},
  eprinttype = {arXiv},
  eprint    = {2103.08862},
  timestamp = {Tue, 23 Mar 2021 16:29:47 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2103-08862.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
%105
@inproceedings{105_li-etal-2022-vision,
    title = "On Vision Features in Multimodal Machine Translation",
    author = "Li, Bei  and
      Lv, Chuanhao  and
      Zhou, Zefan  and
      Zhou, Tao  and
      Xiao, Tong  and
      Ma, Anxiang  and
      Zhu, JingBo",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.438",
    doi = "10.18653/v1/2022.acl-long.438",
    pages = "6327--6337",
    abstract = "Previous work on multimodal machine translation (MMT) has focused on the way of incorporating vision features into translation but little attention is on the quality of vision models. In this work, we investigate the impact of vision models on MMT. Given the fact that Transformer is becoming popular in computer vision, we experiment with various strong models (such as Vision Transformer) and enhanced features (such as object-detection and image captioning). We develop a selective attention model to study the patch-level contribution of an image in MMT. On detailed probing tasks, we find that stronger vision models are helpful for learning translation from the visual modality. Our results also suggest the need of carefully examining MMT models, especially when current benchmarks are small-scale and biased.",
}
%106 vit
@inproceedings{106_vit,
  author    = {Alexey Dosovitskiy and
               Lucas Beyer and
               Alexander Kolesnikov and
               Dirk Weissenborn and
               Xiaohua Zhai and
               Thomas Unterthiner and
               Mostafa Dehghani and
               Matthias Minderer and
               Georg Heigold and
               Sylvain Gelly and
               Jakob Uszkoreit and
               Neil Houlsby},
  title     = {An Image is Worth 16x16 Words: Transformers for Image Recognition
               at Scale},
  booktitle = {9th International Conference on Learning Representations, {ICLR} 2021,
               Virtual Event, Austria, May 3-7, 2021},
  publisher = {OpenReview.net},
  year      = {2021},
  url       = {https://openreview.net/forum?id=YicbFdNTTy},
  timestamp = {Wed, 23 Jun 2021 17:36:39 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/DosovitskiyB0WZ21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
%107 swin transformer
@inproceedings{107_swin_transformer,
  author    = {Ze Liu and
               Yutong Lin and
               Yue Cao and
               Han Hu and
               Yixuan Wei and
               Zheng Zhang and
               Stephen Lin and
               Baining Guo},
  title     = {Swin Transformer: Hierarchical Vision Transformer using Shifted Windows},
  booktitle = {2021 {IEEE/CVF} International Conference on Computer Vision, {ICCV}
               2021, Montreal, QC, Canada, October 10-17, 2021},
  pages     = {9992--10002},
  publisher = {{IEEE}},
  year      = {2021},
  url       = {https://doi.org/10.1109/ICCV48922.2021.00986},
  doi       = {10.1109/ICCV48922.2021.00986},
  timestamp = {Thu, 19 May 2022 16:00:58 +0200},
  biburl    = {https://dblp.org/rec/conf/iccv/LiuL00W0LG21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
%108
@article{108_DBLP:journals/corr/abs-2211-11812,
  author    = {Hanlin Mo and
               Guoying Zhao},
  title     = {{RIC-CNN:} Rotation-Invariant Coordinate Convolutional Neural Network},
  journal   = {CoRR},
  volume    = {abs/2211.11812},
  year      = {2022},
  url       = {https://doi.org/10.48550/arXiv.2211.11812},
  doi       = {10.48550/arXiv.2211.11812},
  eprinttype = {arXiv},
  eprint    = {2211.11812},
  timestamp = {Thu, 02 Feb 2023 16:04:34 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2211-11812.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{109_DBLP:journals/corr/abs-2007-10588,
  author    = {Jinpyo Kim and
               Wookeun Jung and
               Hyungmo Kim and
               Jaejin Lee},
  title     = {CyCNN: {A} Rotation Invariant {CNN} using Polar Mapping and Cylindrical
               Convolution Layers},
  journal   = {CoRR},
  volume    = {abs/2007.10588},
  year      = {2020},
  url       = {https://arxiv.org/abs/2007.10588},
  eprinttype = {arXiv},
  eprint    = {2007.10588},
  timestamp = {Tue, 28 Jul 2020 14:46:12 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2007-10588.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
%110
@article{110_DBLP:journals/jmlr/AzulayW19,
  author    = {Aharon Azulay and
               Yair Weiss},
  title     = {Why do deep convolutional networks generalize so poorly to small image
               transformations?},
  journal   = {J. Mach. Learn. Res.},
  volume    = {20},
  pages     = {184:1--184:25},
  year      = {2019},
  url       = {http://jmlr.org/papers/v20/19-519.html},
  timestamp = {Thu, 18 Jun 2020 22:13:22 +0200},
  biburl    = {https://dblp.org/rec/journals/jmlr/AzulayW19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
%111
@inproceedings{111_ive-etal-2021-exploiting,
    title = "Exploiting Multimodal Reinforcement Learning for Simultaneous Machine Translation",
    author = "Ive, Julia  and
      Li, Andy Mingren  and
      Miao, Yishu  and
      Caglayan, Ozan  and
      Madhyastha, Pranava  and
      Specia, Lucia",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-main.281",
    doi = "10.18653/v1/2021.eacl-main.281",
    pages = "3222--3233",
    abstract = "This paper addresses the problem of simultaneous machine translation (SiMT) by exploring two main concepts: (a) adaptive policies to learn a good trade-off between high translation quality and low latency; and (b) visual information to support this process by providing additional (visual) contextual information which may be available before the textual input is produced. For that, we propose a multimodal approach to simultaneous machine translation using reinforcement learning, with strategies to integrate visual and textual information in both the agent and the environment. We provide an exploration on how different types of visual information and integration strategies affect the quality and latency of simultaneous translation models, and demonstrate that visual cues lead to higher quality while keeping the latency low.",
}
%112
@inproceedings{112_specia-etal-2016-shared,
    title = "A Shared Task on Multimodal Machine Translation and Crosslingual Image Description",
    author = "Specia, Lucia  and
      Frank, Stella  and
      Sima{'}an, Khalil  and
      Elliott, Desmond",
    booktitle = "Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W16-2346",
    doi = "10.18653/v1/W16-2346",
    pages = "543--553",
}
%113
@inproceedings{113_elliott-etal-2017-findings,
    title = "Findings of the Second Shared Task on Multimodal Machine Translation and Multilingual Image Description",
    author = {Elliott, Desmond  and
      Frank, Stella  and
      Barrault, Lo{\"\i}c  and
      Bougares, Fethi  and
      Specia, Lucia},
    booktitle = "Proceedings of the Second Conference on Machine Translation",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W17-4718",
    doi = "10.18653/v1/W17-4718",
    pages = "215--233",
}
%114
@inproceedings{114_barrault-etal-2018-findings,
    title = "Findings of the Third Shared Task on Multimodal Machine Translation",
    author = {Barrault, Lo{\"\i}c  and
      Bougares, Fethi  and
      Specia, Lucia  and
      Lala, Chiraag  and
      Elliott, Desmond  and
      Frank, Stella},
    booktitle = "Proceedings of the Third Conference on Machine Translation: Shared Task Papers",
    month = oct,
    year = "2018",
    address = "Belgium, Brussels",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-6402",
    doi = "10.18653/v1/W18-6402",
    pages = "304--323",
    abstract = "We present the results from the third shared task on multimodal machine translation. In this task a source sentence in English is supplemented by an image and participating systems are required to generate a translation for such a sentence into German, French or Czech. The image can be used in addition to (or instead of) the source sentence. This year the task was extended with a third target language (Czech) and a new test set. In addition, a variant of this task was introduced with its own test set where the source sentence is given in multiple languages: English, French and German, and participating systems are required to generate a translation in Czech. Seven teams submitted 45 different systems to the two variants of the task. Compared to last year, the performance of the multimodal submissions improved, but text-only systems remain competitive.",
}
%115
@inproceedings{115_saha-etal-2016-correlational,
    title = "A Correlational Encoder Decoder Architecture for Pivot Based Sequence Generation",
    author = "Saha, Amrita  and
      Khapra, Mitesh M.  and
      Chandar, Sarath  and
      Rajendran, Janarthanan  and
      Cho, Kyunghyun",
    booktitle = "Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",
    month = dec,
    year = "2016",
    address = "Osaka, Japan",
    publisher = "The COLING 2016 Organizing Committee",
    url = "https://aclanthology.org/C16-1011",
    pages = "109--118",
    abstract = "Interlingua based Machine Translation (MT) aims to encode multiple languages into a common linguistic representation and then decode sentences in multiple target languages from this representation. In this work we explore this idea in the context of neural encoder decoder architectures, albeit on a smaller scale and without MT as the end goal. Specifically, we consider the case of three languages or modalities X, Z and Y wherein we are interested in generating sequences in Y starting from information available in X. However, there is no parallel training data available between X and Y but, training data is available between X {\&} Z and Z {\&} Y (as is often the case in many real world applications). Z thus acts as a pivot/bridge. An obvious solution, which is perhaps less elegant but works very well in practice is to train a two stage model which first converts from X to Z and then from Z to Y. Instead we explore an interlingua inspired solution which jointly learns to do the following (i) encode X and Z to a common representation and (ii) decode Y from this common representation. We evaluate our model on two tasks: (i) bridge transliteration and (ii) bridge captioning. We report promising results in both these applications and believe that this is a right step towards truly interlingua inspired encoder decoder architectures.",
}
%116
@inproceedings{116_DBLP:conf/naacl/HirasawaYMK19,
  author    = {Tosho Hirasawa and
               Hayahide Yamagishi and
               Yukio Matsumura and
               Mamoru Komachi},
  editor    = {Sudipta Kar and
               Farah Nadeem and
               Laura Burdick and
               Greg Durrett and
               Na{-}Rae Han},
  title     = {Multimodal Machine Translation with Embedding Prediction},
  booktitle = {Proceedings of the 2019 Conference of the North American Chapter of
               the Association for Computational Linguistics: Human Language Technologies,
               {NAACL-HLT} 2019, Minneapolis, MN, USA, June 3-5, 2019, Student Research
               Workshop},
  pages     = {86--91},
  publisher = {Association for Computational Linguistics},
  year      = {2019},
  url       = {https://doi.org/10.18653/v1/n19-3012},
  doi       = {10.18653/v1/n19-3012},
  timestamp = {Fri, 06 Aug 2021 00:41:30 +0200},
  biburl    = {https://dblp.org/rec/conf/naacl/HirasawaYMK19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
%117
@article{117_DBLP:journals/corr/abs-1910-02766,
  author    = {Jean{-}Benoit Delbrouck and
               St{\'{e}}phane Dupont},
  title     = {Adversarial reconstruction for Multi-modal Machine Translation},
  journal   = {CoRR},
  volume    = {abs/1910.02766},
  year      = {2019},
  url       = {http://arxiv.org/abs/1910.02766},
  eprinttype = {arXiv},
  eprint    = {1910.02766},
  timestamp = {Wed, 09 Oct 2019 14:07:58 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1910-02766.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
%118
@inproceedings{118_DBLP:conf/iclr/0001C0USLZ20,
  author    = {Zhuosheng Zhang and
               Kehai Chen and
               Rui Wang and
               Masao Utiyama and
               Eiichiro Sumita and
               Zuchao Li and
               Hai Zhao},
  title     = {Neural Machine Translation with Universal Visual Representation},
  booktitle = {8th International Conference on Learning Representations, {ICLR} 2020,
               Addis Ababa, Ethiopia, April 26-30, 2020},
  publisher = {OpenReview.net},
  year      = {2020},
  url       = {https://openreview.net/forum?id=Byl8hhNYPS},
  timestamp = {Thu, 21 Jan 2021 17:36:45 +0100},
  biburl    = {https://dblp.org/rec/conf/iclr/0001C0USLZ20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
%119
@inproceedings{119_fang-feng-2022-neural,
    title = "Neural Machine Translation with Phrase-Level Universal Visual Representations",
    author = "Fang, Qingkai  and
      Feng, Yang",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.390",
    doi = "10.18653/v1/2022.acl-long.390",
    pages = "5687--5698",
    abstract = "Multimodal machine translation (MMT) aims to improve neural machine translation (NMT) with additional visual information, but most existing MMT methods require paired input of source sentence and image, which makes them suffer from shortage of sentence-image pairs. In this paper, we propose a phrase-level retrieval-based method for MMT to get visual information for the source input from existing sentence-image data sets so that MMT can break the limitation of paired sentence-image input. Our method performs retrieval at the phrase level and hence learns visual information from pairs of source phrase and grounded region, which can mitigate data sparsity. Furthermore, our method employs the conditional variational auto-encoder to learn visual representations which can filter redundant visual information and only retain visual information related to the phrase. Experiments show that the proposed method significantly outperforms strong baselines on multiple MMT datasets, especially when the textual context is limited.",
}
%120
@inproceedings{120_tang-etal-2022-multimodal,
    title = "Multimodal Neural Machine Translation with Search Engine Based Image Retrieval",
    author = "Tang, ZhenHao  and
      Zhang, XiaoBing  and
      Long, Zi  and
      Fu, XiangHua",
    booktitle = "Proceedings of the 9th Workshop on Asian Translation",
    month = oct,
    year = "2022",
    address = "Gyeongju, Republic of Korea",
    publisher = "International Conference on Computational Linguistics",
    url = "https://aclanthology.org/2022.wat-1.11",
    pages = "89--98",
    abstract = "Recently, numbers of works shows that the performance of neural machine translation (NMT) can be improved to a certain extent with using visual information. However, most of these conclusions are drawn from the analysis of experimental results based on a limited set of bilingual sentence-image pairs, such as Multi30K.In these kinds of datasets, the content of one bilingual parallel sentence pair must be well represented by a manually annotated image,which is different with the actual translation situation. we propose an open-vocabulary image retrieval methods to collect descriptive images for bilingual parallel corpus using image search engine, and we propose text-aware attentive visual encoder to filter incorrectly collected noise images. Experiment results on Multi30K and other two translation datasets show that our proposed method achieves significant improvements over strong baselines.",
}
%121
@inproceedings{121_DBLP:conf/icml/GuuLTPC20,
  author    = {Kelvin Guu and
               Kenton Lee and
               Zora Tung and
               Panupong Pasupat and
               Ming{-}Wei Chang},
  title     = {Retrieval Augmented Language Model Pre-Training},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning,
               {ICML} 2020, 13-18 July 2020, Virtual Event},
  series    = {Proceedings of Machine Learning Research},
  volume    = {119},
  pages     = {3929--3938},
  publisher = {{PMLR}},
  year      = {2020},
  url       = {http://proceedings.mlr.press/v119/guu20a.html},
  timestamp = {Tue, 15 Dec 2020 17:40:18 +0100},
  biburl    = {https://dblp.org/rec/conf/icml/GuuLTPC20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
%122
@inproceedings{122_DBLP:conf/emnlp/WestonDM18,
  author    = {Jason Weston and
               Emily Dinan and
               Alexander H. Miller},
  editor    = {Aleksandr Chuklin and
               Jeff Dalton and
               Julia Kiseleva and
               Alexey Borisov and
               Mikhail S. Burtsev},
  title     = {Retrieve and Refine: Improved Sequence Generation Models For Dialogue},
  booktitle = {Proceedings of the 2nd International Workshop on Search-Oriented Conversational
               AI, SCAI@EMNLP 2018, Brussels, Belgium, October 31, 2018},
  pages     = {87--92},
  publisher = {Association for Computational Linguistics},
  year      = {2018},
  url       = {https://doi.org/10.18653/v1/w18-5713},
  doi       = {10.18653/v1/w18-5713},
  timestamp = {Fri, 06 Aug 2021 00:40:25 +0200},
  biburl    = {https://dblp.org/rec/conf/emnlp/WestonDM18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
%123
@inproceedings{123_DBLP:conf/iclr/KhandelwalLJZL20,
  author    = {Urvashi Khandelwal and
               Omer Levy and
               Dan Jurafsky and
               Luke Zettlemoyer and
               Mike Lewis},
  title     = {Generalization through Memorization: Nearest Neighbor Language Models},
  booktitle = {8th International Conference on Learning Representations, {ICLR} 2020,
               Addis Ababa, Ethiopia, April 26-30, 2020},
  publisher = {OpenReview.net},
  year      = {2020},
  url       = {https://openreview.net/forum?id=HklBjCEKvH},
  timestamp = {Thu, 07 May 2020 17:11:47 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/KhandelwalLJZL20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
%124
@inproceedings{124_DBLP:conf/aaai/GuWCL18,
  author    = {Jiatao Gu and
               Yong Wang and
               Kyunghyun Cho and
               Victor O. K. Li},
  editor    = {Sheila A. McIlraith and
               Kilian Q. Weinberger},
  title     = {Search Engine Guided Neural Machine Translation},
  booktitle = {Proceedings of the Thirty-Second {AAAI} Conference on Artificial Intelligence,
               (AAAI-18), the 30th innovative Applications of Artificial Intelligence
               (IAAI-18), and the 8th {AAAI} Symposium on Educational Advances in
               Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February
               2-7, 2018},
  pages     = {5133--5140},
  publisher = {{AAAI} Press},
  year      = {2018},
  url       = {https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17282},
  timestamp = {Tue, 08 Mar 2022 21:46:35 +0100},
  biburl    = {https://dblp.org/rec/conf/aaai/GuWCL18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
%125
@inproceedings{125_DBLP:conf/aaai/ChenLL18,
  author    = {Yun Chen and
               Yang Liu and
               Victor O. K. Li},
  editor    = {Sheila A. McIlraith and
               Kilian Q. Weinberger},
  title     = {Zero-Resource Neural Machine Translation with Multi-Agent Communication
               Game},
  booktitle = {Proceedings of the Thirty-Second {AAAI} Conference on Artificial Intelligence,
               (AAAI-18), the 30th innovative Applications of Artificial Intelligence
               (IAAI-18), and the 8th {AAAI} Symposium on Educational Advances in
               Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February
               2-7, 2018},
  pages     = {5086--5093},
  publisher = {{AAAI} Press},
  year      = {2018},
  url       = {https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16709},
  timestamp = {Tue, 08 Mar 2022 21:46:35 +0100},
  biburl    = {https://dblp.org/rec/conf/aaai/ChenLL18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
%126
@inproceedings{126_DBLP:conf/cvpr/SuFBKH19,
  author    = {Yuanhang Su and
               Kai Fan and
               Nguyen Bach and
               C.{-}C. Jay Kuo and
               Fei Huang},
  title     = {Unsupervised Multi-Modal Neural Machine Translation},
  booktitle = {{IEEE} Conference on Computer Vision and Pattern Recognition, {CVPR}
               2019, Long Beach, CA, USA, June 16-20, 2019},
  pages     = {10482--10491},
  publisher = {Computer Vision Foundation / {IEEE}},
  year      = {2019},
  url       = {http://openaccess.thecvf.com/content\_CVPR\_2019/html/Su\_Unsupervised\_Multi-Modal\_Neural\_Machine\_Translation\_CVPR\_2019\_paper.html},
  doi       = {10.1109/CVPR.2019.01073},
  timestamp = {Fri, 02 Sep 2022 18:51:28 +0200},
  biburl    = {https://dblp.org/rec/conf/cvpr/SuFBKH19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
%127
@inproceedings{127_huang-etal-2020-unsupervised-multimodal,
    title = "Unsupervised Multimodal Neural Machine Translation with Pseudo Visual Pivoting",
    author = "Huang, Po-Yao  and
      Hu, Junjie  and
      Chang, Xiaojun  and
      Hauptmann, Alexander",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.731",
    doi = "10.18653/v1/2020.acl-main.731",
    pages = "8226--8237",
    abstract = "Unsupervised machine translation (MT) has recently achieved impressive results with monolingual corpora only. However, it is still challenging to associate source-target sentences in the latent space. As people speak different languages biologically share similar visual systems, the potential of achieving better alignment through visual content is promising yet under-explored in unsupervised multimodal MT (MMT). In this paper, we investigate how to utilize visual content for disambiguation and promoting latent space alignment in unsupervised MMT. Our model employs multimodal back-translation and features pseudo visual pivoting in which we learn a shared multilingual visual-semantic embedding space and incorporate visually-pivoted captioning as additional weak supervision. The experimental results on the widely used Multi30K dataset show that the proposed model significantly improves over the state-of-the-art methods and generalizes well when images are not available at the testing time.",
}

%---------------------------------------------------------------------------%

